var searchDocuments = {"html/index.html":{"action-uuid":"b1ab72d2-7b5c-48b4-a189-70c5861f332b","title":"HTML","content":"TODO"},"html/models/app/index.html":{"path":"HTML/Models/App","action-uuid":"cf96917b-0778-4e9d-b59a-d7220777b1d4","title":"App","content":"TODO"},"html/models/bootstrap/index.html":{"path":"HTML/Models/Bootstrap","action-uuid":"79c7e42b-acec-40ac-9560-e5d184acd0fa","title":"Bootstrap","content":"TODO"},"core/maven/index.html":{"path":"Core/Maven","action-uuid":"43747d78-b893-4810-93ee-aecc72a9eb01","title":"Maven","content":"TODO"},"core/persistence/index.html":{"path":"Core/Persistence","action-uuid":"d42a246e-7b2e-4bcd-b046-d1e0930e0b2e","title":"Persistence","content":"TODO"},"core/diagram/index.html":{"path":"Core/Diagram","action-uuid":"772502ad-8902-4244-88d7-8982b2f3e583","title":"Diagram","content":"TODO"},"html/emf/index.html":{"path":"HTML/EMF","action-uuid":"16dd87c3-fca7-403e-b871-ce65b367b19a","title":"EMF","content":"TODO"},"html/bootstrap/index.html":{"path":"HTML/Bootstrap","action-uuid":"324a6fc2-94e0-42ae-a3ec-39d6710cc2f8","title":"Bootstrap","content":"TODO"},"core/resources/index.html":{"path":"Core/Resources","action-uuid":"8c9f3c1c-bef9-4acc-bb88-50828214728f","title":"Resources","content":"TODO"},"practices/index.html":{"action-uuid":"e9d19b9d-9aa9-46f6-90e0-6ef25aa2fd22","title":"Practices","content":"Practices explain how to use Nasdanika products to achieve specific goals and explain why particular design choices wer made. The enterprise model provides deeper insight on the WHY in general. The practices are organized into an enterprise continuum from the most generic on the left to the most specific on the right. However, the most specific on the right is still generic and needs to be specialized for a particular application (embodiment): Analysis, Visualization &amp; Generation - describes a general approach on using Nasdanika products. Java Analysis, Visualization &amp; Generation - application of the above to the Java model1 Loading and analyzing Java sources and bytecode, generation of non-Java artifacts such as HTML reports Generation of Java sources. JUnit test generation for low coverage methods - further specialization of the Java practice to identify methods with low test coverage using the Coverage Model and then generate JUnit tests for those methods using the Java model and OpenAI. You can think of the three practices above as progressive &ldquo;binding of decision&rdquo; as you move from the left to the right to reach &ldquo;executability&rdquo; - ability to deliver value. A java analogy for progressive specialization would be incremental binding of generic types as exemplified below: Map&lt;K,V&gt; - generic map. MyMap&lt;K extends Comparable&gt; extends Map&lt;&lt;K, MyValue&lt;K&gt;&gt; - the above map bound to a single generic parameter with an upper bound. It is a specialization of the above generic map which is also generic. Some decisions were bound, but there are still decisions to be bound. MyMap&lt;String&gt; theMap = ...; - fully bound map. Decisions are bound at variation point. For example, &ldquo;storage&rdquo; is a variation point, &ldquo;blob storage&rdquo; is one of alternatives, decision to use &ldquo;blob storage&rdquo; binds the variation point to a specific alternative. Decision binding forms a graph. Once you bind, say, &ldquo;storage&rdquo; variation point, some downstream alternatives may become unavailable because they are incompatible with that binding. Some might be available, but make no sense. For example, a decision to send data unencrypted over a public network is compatible with a decision to purchase some additional strong encryption hardware to use on-prem, but does it make business sense? Different alternatives feature different &ldquo;quality attributes&rdquo; - performance, reliability, cost. As the number of variation points and alternatives grows purely human-based decision making becomes inefficient. In this case variation points can be modeled as requirements and alternatives as capability providers or capabilities with quality attributes (seecapability). After this a list of &ldquo;designs&rdquo; (a.k.a. &ldquo;provisioning plans&rdquo;) can be created. A design/provisioning plan is a collection of compatible capabilities. If a list of designs is short enough it can be analyzed by humans directly. In the case of long lists or a large number of very similar designs decision analysis can be employed for making a selection of a design which is best fit for purpose. The page provides a general overview and the book goes into more details. â†©"},"core/emf/index.html":{"path":"Core/EMF","action-uuid":"78fa019c-4509-4c25-9dfd-270ff0873417","title":"EMF","content":"TODO"},"html/models/html/index.html":{"path":"HTML/Models/HTML","action-uuid":"9b59e7c9-4c28-4b7a-aa1b-8d437c92ddbc","title":"HTML","content":"TODO"},"index.html":{"action-uuid":"6d411d79-ae77-4e7a-8c40-42ecdf06305b","title":"Nasdanika","content":" Common Resources Persistence Ncore Diagram Graph Drawio EMF Exec Maven Capability Core HTML HTML Bootstrap App Models JsTree Bootstrap EMF HTML GitLab Family Architecture Git Excel ECharts Nature Bank PDF Party Coverage Source Engineering Java Maven Enterprise Function Flow Rules Azure Decision Analysis Capability Flow Ecore Jira Models Data Sources Loader Store Key Extractor Query Engine Requestor Generator Responder Retrieval Augmented Generation (RAG) Analysis, Visualization &amp; Generation Java Analysis, Visualization &amp; Generation JUnit Tests Generation Practices Beyond Diagrams Java Analysis, Visualization, and Generation Books Common Resources Persistence Ncore Diagram Graph Drawio EMF Exec Maven Capability Core HTML HTML Bootstrap App Models JsTree Bootstrap EMF HTML GitLab Family Architecture Git Excel ECharts Nature Bank PDF Party Coverage Source Engineering Java Maven Enterprise Function Flow Rules Azure Decision Analysis Capability Flow Ecore Jira Models Data Sources Loader Store Key Extractor Query Engine Requestor Generator Responder Retrieval Augmented Generation (RAG) Analysis, Visualization &amp; Generation Java Analysis, Visualization &amp; Generation JUnit Tests Generation Practices Beyond Diagrams Java Analysis, Visualization, and Generation Books"},"html/jstree/index.html":{"path":"HTML/JsTree","action-uuid":"0ba4024b-706f-4b4b-b5c3-3deb9a1d9435","title":"JsTree","content":"TODO"},"glossary.html":{"action-uuid":"21dcbf69-4331-46e7-85a6-d24e0366b355","title":"Glossary","content":"Clear Identifier(s) Hide UUID {{data.value.name}} {{data.value[0].value}} {{item.value}}"},"core/common/index.html":{"path":"Core/Common","action-uuid":"a0294ba0-5aa9-492d-a3c6-b7ebfd8304bb","title":"Common","content":"TODO"},"practices/junit/index.html":{"path":"Practices/JUnit Tests Generation","action-uuid":"cc797caa-e4b5-46c7-a3de-6a67f4965c6c","title":"JUnit Tests Generation","content":"This practice is a specialization of the Java Analysis, Visualization &amp; Generation Practice for generation of JUnit tests. In particular: Generation of tests for methods or classes with low test coverage Leveraging Gen AI such as OpenAI ChatGPT or Azure OpenAI Service for test generation The above diagram shows Java development activities and artifacts. Black arrows show the typical process, blue arrows show the test generation loop. The developer produces source artifacts which may include non-java artifacts used to generate Java code (e.g. Ecore models), &ldquo;main&rdquo; Java sources and test Java sources. Java sources are compiled into bytecode (class files). Here it is important to note that matching of bytecode classes and methods to source code classes and methods might be non-trivial because of: Lambdas Anonymous and method-scope classes Annotation processors like Lombok JUnit tests are compiled and executed. If code coverage, such as jacoco, is configured then test execution produces coverage data. Jacoco stores coverage data in jacoco.exec file. This file is used to generate a coverage report and upload coverage information to systems like SonarQube. In this practice it is also used to select which methods to generate tests for based on coverage data. This diagram provides an insight into the test generation activity: Coverage data and bytecode are used as input to load the Coverage model. Source files, the coverage model, and bytecode (optional) are used to load the Java model of source code. The generator traverses the model and generates unit tests for method with low coverage using a combination of programmatic (traditional) generation and Gen AI. Tests are generated as a Java model as well and then are delivered to the developer for review, modification, and inclusion into the unit test suite. The following section provides an overview of two &ldquo;local loop&rdquo; reference implementations (a.k.a. designs/embodiments) - all-in-one and componentized. There are many possible designs leveraging different alternatives at multiple variation points. The sections after the reference implementations section provide an overview of variation points, alternatives, and factors to take into consideration during alternative selection. Reference Implementations All-in-one Componentized Variation points and alternatives Tests generator Delivery of generated tests Test generation execution Access to artifacts Parking lot Reference Implementations This section explains two reference implementations All-in-one All-in-one generations is implemented as a JUnit test is available in TestGenerator. An example of tests generated by this generator - PetControllerTests. As the name implies, all steps of source analysis and generation are implemented in a single class and are executed in one go. Componentized Componentized test generation which is also executed in one go is implemented in these classes: TestJavaAnalyzers - loads sources, coverage, and inspectors, passes the sources to the inspectors, aggregates and saves results. Coverage Inspector - generates tests for methods with low coverage leveraging TestGenerator capability provided by OpenAITestGenerator. Variation points and alternatives Tests generator Delivery of generated tests Test generation execution Maven plugin, pipeline, crawling Access to artifacts Jenkins workspace, Maven repository, Maven classloader / artifacts loader - reference Maven model TODO: Diagram (site - activities, artifacts, variation points), manually created sources, generated source, bytecode, coverage report. Lombok, ecore. Sorting (by dependency, size, &hellip;), throttling, creation of work items Comparison, positioning, complementary - availability of IDE plug-ins/extensions in org. Not better, different. IDE plug-ins: select code, type a request, wait for generation, save to a file, add package declaration and some imports. dependency: top-down - downstream dependencies might be tested along the way bottom-up both have merits and might be applicable in diffent situations. bottom-up for new development, top-down for addressing technical debt. Parking lot Mention GitLab URI handler - get from, push to, create a merge reguest, link to sources Fine-tuned models (?) Llama 2? Massaging returned text - backticks, imports - explicit and implicit, use Java parser/model. Pet clinic demo - G/H fork, use GPT 4, specify SpringBoot and Mockito in prompt, tags - as is/disabled test methods Use with G/H copilot - complementary, different Unit tests delivery - branch, fork SonarQube - class level. jacoco.exec - Maven repository - evidence for other purposes. Coverage storage. Jenkins workspace Mention assembly of a solution from alternatives, use capability and decision analysis. Coverage - jacoco.exec + class files, sq api, JSON, DB/REST API (part of the enterprise model, mention the generic thing), XMI/Binary. Storage - local, Jenkins workspace, sq, maven binary repository Mock opportunities - we know methods being called, constructors etc. We can generate mock generation and try/catch for static methods and constructors. Branches, switch cases in particular and enums Generate review/explain @Mock, @InjectMocks Cost value for each acitivity, opportunities Sort by dependency, lower-level may be (partially) testes as part of higher level Prompt building - eContainer, resource, resource set (on-demand loading), mention enterprise model from the generic Massaging generated sources - peeling back-ticks, parsing, imports - in the snippet, implicit/assumed. All in one code snippet, rules - diagram and snippets Generation - lombok Recipe book Loops - local, pipeline, issues, scan repos, pull/merge request, sq TODO - sequence diagram for the componentized Java Sources Source Artifacts Bytecode Coverage Data Developer JUnit Tests Gen AI Code Generation Compilation Test Execution JUnit Test Generation Coverage Report Java Sources Bytecode Coverage Data Developer Gen AI Coverage Model Source Code Model Tests Model Generator JUnit Test Generation"},"core/drawio/index.html":{"path":"Core/Drawio","action-uuid":"ddef6eb8-0d51-4e0f-bda4-97e6dc996720","title":"Drawio","content":"Nasdankia provides two Maven modules for working with Drawio diagrams - API and Model. The modules require Java 17 or above. API Drawio module provides Java API for reading and manipulating Drawio diagrams. It is built on top of Graph. The module provides the following interfaces representing elements of a diagram file: Document - the root object of the API representing a file/resource which contains one or more pages. Page - a page containing a diagram (Model). Model - a diagram model containing the diagram root. Root - the root of the model containing layers. Layer - a diagram may have one or more layers. Layers contain Nodes and Connections. Node - a node can be connected to other nodes with connections. A node may contain other nodes and connections. Connection - a connection between two nodes. The below diagram shows relationships between the above interfaces including their super-interfaces: Util provides utility methods such as layout() and methods to navigate and query documents and their elements. Model Drawio Model module provides an EMF Ecore model for diagrams. A model instance can be obtained from the API document by calling Document.toModelDocument() method. The model makes it more convenient to work with the diagram elements by: Making a reference between pages and model elements bi-directional. Introducing Tag class as opposed to a string in the API. Tag is contained by Page and has bi-directional reference with tagged elements. Document The root object of the API representing a file/resource which contains one or more pages Page A page containing a diagram (Model) Model A diagram model containing the diagram root Root The root of the model containing layers Layer A diagram may have one or more layers. Layers contain Nodes and Connections. Link [0..1] Layer Element Element Model Element Node A node can be connected to other nodes with connections. A node may contain other nodes and connections. Connection A connection between two nodes * source 0..1 outgoingConnections * 1 1 1..* * target 0..1 incomingConnections * Tag * * *"},"core/exec/index.html":{"path":"Core/Exec","action-uuid":"c8a7bf54-6b86-469f-a7c4-c24c5295c23d","title":"Exec","content":"TODO"},"core/ncore/index.html":{"path":"Core/Ncore","action-uuid":"f11c25a0-49f2-41af-b466-c76a39085d86","title":"Ncore","content":"TODO"},"html/html/index.html":{"path":"HTML/HTML","action-uuid":"e2da7a8a-64f3-4dd2-b3ce-e0461b724e9c","title":"HTML","content":"TODO"},"practices/java/index.html":{"path":"Practices/Java Analysis, Visualization &amp; Generation","action-uuid":"a454aea3-79e2-4156-bebf-78fbb96d0095","title":"Java Analysis, Visualization &amp; Generation","content":"sequence diagrams; method, class, package, module dependency graphs. Size and coverage - red border, green core. Logarithmic or scaled - min/max. Doc similar to model doc. GenAI explain and recommend - legacy code Mention the book - this is a high level overview, book is more detaied merging/ranges Sources linked to the org/architecture model - chat with them (RAG)"},"core/capability/index.html":{"path":"Core/Capability","action-uuid":"1d8bb0e8-c150-48f6-a7d8-43df1e4f500d","title":"Capability","content":"Nasdanika Capability framework allows to discover/load capabilities which meet a requirement. Capabilities are provided by CapabilityFactory create() method. Capability factories may request other capabilities they need. As such, capabilities can be chained. Factories create CapabilityLoaders which provide Flux reactive streams of capabilities. It allows to have an infinite stream of capabilities which are consumed (and produced) as needed. Capability providers may furnish additional information about capabilities. This information can be used for filtering or sorting providers. A non-technical example of requirement/capability chain graph is a food chain/graph. Food is a requirement. Or &ldquo;I want to eat&rdquo; is a requirement. Bread and, say fried eggs are two capabilities meeting/addressing the requirement. Bread requires &ldquo;wheat&rdquo;, &ldquo;water&rdquo;, and &ldquo;bake&rdquo; capabilities. Fried eggs require &ldquo;egg&rdquo;, &ldquo;oil&rdquo;, and &ldquo;fry&rdquo; capabilities. Bread capability provider may implement Vegan marker interface which can be used for filtering. All food capabilities may implement NutritionalInformation interface - it can be used for filtering or sorting. A more technical example is Java ServiceLoader with service type being a requirement and an instance of the service class being a capability. Nasdanika capability framework can operate on top of ServiceLoader and may be thought of as a generalization of service loading. In essence, the capability framework is a backward chaining engine as shown in one of the example below. Client code - requesting a capability Capabilities are loaded by CapabilityLoader. Capability loader can take an iterable of capability factories in its constructor, or it can load them using ServiceLoader as shown in the below code snippet: CapabilityLoader capabilityLoader = new CapabilityLoader();\ncapabilityLoader.getFactories().add(new TestServiceFactory&lt;Object&gt;());\nProgressMonitor progressMonitor = new PrintStreamProgressMonitor();\n\t\t\nfor (CapabilityProvider&lt;?&gt; cp: capabilityLoader.load(new TestCapabilityFactory.Requirement(&quot;Hello World&quot;), progressMonitor)) {\n\tSystem.out.println(cp);\n\tFlux&lt;?&gt; publisher = cp.getPublisher();\n\t\t\t\n\tpublisher.subscribe(System.out::println);\n}\n Factories can also be added post-construction with getFactories().add(factory). Service capabilities Service requirements and capabilities provide functionality similar to ServiceLoader - requesting instances of specific type, but extend it with ability to provide additional service requirement. This functionality is provided by ServiceCapabilityFactory and ServiceCapabilityFactory.Requirement. CapabilityLoader capabilityLoader = new CapabilityLoader();\ncapabilityLoader.getFactories().add(new TestServiceFactory&lt;Object&gt;());\nProgressMonitor progressMonitor = new PrintStreamProgressMonitor();\n\t\t\n@SuppressWarnings({ &quot;unchecked&quot;, &quot;rawtypes&quot; })\nServiceCapabilityFactory.Requirement&lt;List&lt;Double&gt;, Double&gt; requirement = (ServiceCapabilityFactory.Requirement) ServiceCapabilityFactory.createRequirement(List.class, null,  33.0);\nfor (CapabilityProvider&lt;?&gt; cp: capabilityLoader.load(requirement, progressMonitor)) {\n\tSystem.out.println(cp);\n\tFlux&lt;?&gt; publisher = cp.getPublisher();\n\t\t\t\n\tpublisher.subscribe(System.out::println);\n}\n It is also possible to load services from ServiceLoader using subclasses of Service. You&rsquo;d need to subclass ServiceFactory in a module which uses a particular service and override stream(Class&lt;S&gt; service) method as shown below: @Override\nprotected Stream&lt;Provider&lt;S&gt;&gt; stream(Class&lt;S&gt; service) {\n\treturn ServiceLoader.load(service).stream();\n}\n Then you&rsquo;d need to add the factory to the loader: capabilityLoader.getFactories().add(new TestServiceFactory&lt;Object&gt;());\n Providing a capability As it was mentioned above, capability factories can be explicitly added to CapabilityLoader or loaded using ServiceLoader. Below is an example of a capability factory: public class TestCapabilityFactory implements CapabilityFactory&lt;TestCapabilityFactory.Requirement, Integer&gt; {\n\t\n\tpublic record Requirement(String value){};\n\t\n\t@Override\n\tpublic boolean canHandle(Object requirement) {\n\t\treturn requirement instanceof Requirement;\n\t}\n\n\t@Override\n\tpublic CompletionStage&lt;Iterable&lt;CapabilityProvider&lt;Integer&gt;&gt;&gt; create(\n\t\t\tRequirement requirement,\n\t\t\tBiFunction&lt;Object, ProgressMonitor, CompletionStage&lt;Iterable&lt;CapabilityProvider&lt;Object&gt;&gt;&gt;&gt; resolver,\n\t\t\tProgressMonitor progressMonitor) {\n\t\t\n\t\treturn resolver.apply(MyService.class, progressMonitor).thenApply(cp -&gt; {;\n\t\t\t@SuppressWarnings({ &quot;unchecked&quot;, &quot;rawtypes&quot; })\n\t\t\tFlux&lt;MyService&gt; myServiceCapabilityPublisher = (Flux) cp.iterator().next().getPublisher();\n\t\t\t\n\t\t\treturn Collections.singleton(new CapabilityProvider&lt;Integer&gt;() {\n\t\n\t\t\t\t@Override\n\t\t\t\tpublic Flux&lt;Integer&gt; getPublisher() {\n\t\t\t\t\tFunction&lt;MyService, Integer&gt; mapper = ms -&gt; ms.count(((Requirement) requirement).value());\n\t\t\t\t\treturn myServiceCapabilityPublisher.map(mapper);\n\t\t\t\t}\n\t\t\t\t\n\t\t\t});\n\t\t});\n\t}\n\n}\n Applications Services Service capabilities explained above a used by Graph and Function Flow for loading node processors and connection processors for a specific requirement using NodeProcessorFactory and ConnectionProcessorFactory respectively. For example, code generation, execution, simulation. Solutions for architectures One of future application of the capability framework is creation a list of solution alternatives for an architecture/pattern. For example, there might be multiple RAG embodiments with different key types, key extractors, stores, &hellip; Some of &ldquo;design dimensions&rdquo; are listed below: Key type: Bag of words. Multiple options - just words, words with frequency, tokenized words, word stems. Embedding vector - different embedding models, different dimensions. Store - multiple stores for multiple key types. Multiple indexing and retrieval methods. Chunk size, chunk overlap, chunking algorithm. Generator - multiple models and prompts As you can see a number of potential combinations can easily go into thousands or even be infinite. Reactive approach with filtering and sorting may be helpful in selecting a solution which is a good fit for a particular use case - number and type of data sources etc. For example, if the total size of data is under a few gigabytes an in-memory store may be a better choice than, say, an external (vector) database. Also an old good bag of words might be better than embeddings. E.g. it might be cheaper. Solution alternatives may include temporal aspect or monetary aspects. For example, version X of Y is available at time Z. Z might be absolute or relative. Say, Z days after project kick-off or license fee payment. Identified solutions meeting requirements can have different quality attributes - costs (to build, to run), timeline, etc. These quality attributes can be used for solution analysis. E.g. one solution can be selected as a transition architecture and another as the target architecture. Backward chaining Family reasoning demonstrates application of the capability framework as a backward chaining engine. Family relationships such as grandfather and cousin are constructed by requiring and combining relationships such as child and sibling. Stream processing This possible application is similar to backward reasoning. Imagine an algorithmic trading strategy which uses several technical indicators, such as moving averages, to make trading decisions. Such a strategy would submit requirements for technical indicators which would include symbol, indicator configuration, time frame size. Technical indicators in turn would submit a requirement for raw trading data. A technical indicator such as moving average would start publishing its events once it receives enough trading data frames to compute its average. A trading engine would submit a requirement for strategies. A strategy factory may produce multiple strategies with different configurations. The trading engine would perform &ldquo;paper&rdquo; trades, select well-performing strategies and discard ones which perform poorly. This can be an ongoing process - if a strategy deteriorates then it is discarded and a new strategy is requested from strategy publishers - this process can be infinite. AI model training/fine-tuning This application is similar to stream processing and may be combined with backward reasoning. Let&rsquo;s say we want to train a model to answer questions about family relationships for a specific family. For example, &ldquo;Who is Alan&rsquo;s great grandmother?&rdquo; A single relationship in the model can be expressed in multiple ways in natural language. And multiple relationships can be expressed in a single sentence. For example: Elias is a person Elias is a man Elias is a male Elias is a parent of Fiona Fiona is a child of Elias Elias is a father of Fiona Fiona is a daughter of Elias Paul and Isa are parents of Lea and Elias &hellip; So, on top of a model there might be a collection of text generators. Output of those generators can be fed to a model: Supervised - question and answer &ldquo;How many sisters does Bryan have?&rdquo; - &ldquo;Two&rdquo; &ldquo;Who are Bryan&rsquo;s sisters?&rdquo; - &ldquo;Clara and Fiona&rdquo; Unsupervised - factual statements A similar approach can be applied to other models - customer/accounts, organization or architecture model, etc. For example, from the Internet Banking System we can generate something like &ldquo;Accounts Summary Controller uses Mainframe Banking System Facade to make API calls to the Mainframe Banking System over XML/HTTPS&rdquo;. &ldquo;make API calls&rdquo; may also be generated as &ldquo;connect&rdquo; or &ldquo;make requests&rdquo;. In a similar fashion a number of questions/answers can be generated."},"core/graph/index.html":{"path":"Core/Graph","action-uuid":"07e41933-9a79-43ff-9a29-72cb507939f1","title":"Graph","content":"TODO"},"core/index.html":{"action-uuid":"19d134cb-7da2-4a97-9ad4-8a8b6a9e413a","title":"Core","content":"TODO"},"html/models/index.html":{"path":"HTML/Models","action-uuid":"675ec0c5-4738-4837-9f9d-f61f4f8d6fe1","title":"Models","content":"TODO"},"practices/generic/index.html":{"path":"Practices/Analysis, Visualization &amp; Generation","action-uuid":"12f9b7b3-eb7b-4bf3-870d-a4d84af44b03","title":"Analysis, Visualization &amp; Generation","content":"Decision binding, enterprise model, &hellip; Variation points, designs (provisioning plans), decision analysis - alternatives, quality attributes as criteria input -&gt; process -&gt; output raw data -&gt; model -&gt; process -&gt; model -&gt; output Cross-referencing, e.g. Excel cell referencing a PDF paragraph &hellip; Inheritance/specialization, e.g. Java source (comp unit) in GitLab. Org model. Loading/storing - GitLab URI converter. a brief overview of models, alphabetical conversion to graphs, processors, visualizations Evans, DDD, anti-corruption layer. Just processing - the what, Ecore &amp; NSD are how Inheritance - https://pubs.opengroup.org/togaf-standard/introduction/Figures/34_contentfwk8.png, https://pubs.opengroup.org/togaf-standard/applying-the-adm/Figures/40_partitioning5.png, https://pubs.opengroup.org/togaf-standard/applying-the-adm/chap04.html#tag_04, https://pubs.opengroup.org/togaf-standard/introduction/chap03.html#tag_03_12_03 Create a diagram, roll-up"}}