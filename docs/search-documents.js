var searchDocuments = {"html/index.html":{"action-uuid":"ace86cd4-af92-4f7c-97b7-7c1a4f5e69f0","title":"HTML","content":"TODO"},"html/models/app/index.html":{"path":"HTML/Models/App","action-uuid":"8b7200f5-cee9-449b-a8b3-37e61b43be15","title":"App","content":"TODO"},"nsd-cli/nsd/launcher/index.html":{"path":"CLI/nsd/launcher","action-uuid":"416e9c2f-8437-4ac3-bdc1-76545d395bfe","title":"launcher","content":"Version: org.nasdanika.cli@2024.5.0 \r\nUsage: nsd launcher [-hstvV] [-a=&lt;args&gt;] [-b=&lt;base&gt;] [-c=&lt;className&gt;]\r\n                    [-C=&lt;classPathModules&gt;] [-f=&lt;optionsFile&gt;]\r\n                    [-j=&lt;javaCommand&gt;] [-m=&lt;moduleName&gt;] [-M=&lt;modulesFile&gt;]\r\n                    [-o=&lt;output&gt;] [-p=&lt;pathSeparator&gt;] [-P=&lt;prefix&gt;]\r\n                    [-r=&lt;rootModules&gt;] [&lt;repositories&gt;...]\r\nGenerates Java command line from directories of modules/jars\r\n      [&lt;repositories&gt;...]    Directories to scan for modules,\r\n                             defaults to lib\r\n  -a, --args=&lt;args&gt;          Arguments,\r\n                             defaults to %*\r\n  -b, --base=&lt;base&gt;          Base repositories directory\r\n  -c, --class=&lt;className&gt;    Application class,\r\n                             defaults to org.nasdanika.cli.Application\r\n  -C, --claspath-modules=&lt;classPathModules&gt;\r\n                             Comma-separated list of classpath modules\r\n  -f, --options-file=&lt;optionsFile&gt;\r\n                             File to output options to\r\n  -h, --help                 Show this help message and exit.\r\n  -j, --java=&lt;javaCommand&gt;   Java command,\r\n                             defaults to java\r\n  -m, --module=&lt;moduleName&gt;  Application module,\r\n                             defaults to org.nasdanika.cli\r\n  -M, --modules=&lt;modulesFile&gt;\r\n                             Modules to add to the module path\r\n  -o, --output=&lt;output&gt;      Output file\r\n  -p, --path-separator=&lt;pathSeparator&gt;\r\n                             Path separator,\r\n                             defaults to the system path separator\r\n  -P, --prefix=&lt;prefix&gt;      Module path prefix\r\n  -r, --root-modules=&lt;rootModules&gt;\r\n                             Comma-separated list of root modules\r\n                             Supports .* and .** patterns\r\n  -s, --absolute             Use absolute paths\r\n  -t, --options              Output only options\r\n  -v, --verbose              Output debug invformation\r\n  -V, --version              Print version information and exit."},"html/models/bootstrap/index.html":{"path":"HTML/Models/Bootstrap","action-uuid":"914c36a4-f00f-45f0-8c83-0bd2e89d2610","title":"Bootstrap","content":"TODO"},"core/maven/index.html":{"path":"Core/Maven","action-uuid":"95c67316-5aeb-4057-9c5e-51b3da448ac2","title":"Maven","content":"Javadoc"},"core/persistence/index.html":{"path":"Core/Persistence","action-uuid":"a10485cc-9478-4332-8232-f0b9f35f6b97","title":"Persistence","content":"Javadoc"},"core/diagram/index.html":{"path":"Core/Diagram","action-uuid":"7fca1934-acf1-4f28-b838-e335c6a3a0d5","title":"Diagram","content":"Javadoc"},"html/emf/index.html":{"path":"HTML/EMF","action-uuid":"b5d5b14d-137b-49da-bcf6-d426b9ab780f","title":"EMF","content":"TODO"},"html/bootstrap/index.html":{"path":"HTML/Bootstrap","action-uuid":"6f26c03e-6e04-42a9-8dc3-a6a7c36e3f6e","title":"Bootstrap","content":"TODO"},"nsd-cli/nsd/app/index.html":{"path":"CLI/nsd/app","action-uuid":"e06309ac-9d53-4fc4-84d4-6b7ee2d462ba","title":"app","content":"Version: org.nasdanika.html.model.app.gen.cli@2024.5.0 \r\nUsage: nsd app [-hV] [COMMAND]\r\nHTML Application model commands\r\n  -h, --help      Show this help message and exit.\r\n  -V, --version   Print version information and exit.\r\nCommands:\r\n  site  Generates HTML site"},"core/cli/index.html":{"path":"Core/CLI","action-uuid":"2e207843-b583-42fb-8244-c5cb7e4d2e28","title":"CLI","content":"Classes in this module allow to declaratively construct command line interfaces. It uses picocli to execute commands and capability framework to collect sub-commands and mix-ins. This way command line interfaces can be constructed top-down (default picocli functionality) - parent commands explicitly define sub-commands, and bottom-up - sub-commands are added to parent commands by the framework. Top-down construction can be done using out-the-box picocli capabilities - programmatic add and annotations. Both top-down and bottom-up construction can be done using the capability framework which allows sub-commands/mix-ins to request capabilities they need and add themselves to parent commands only if all requirements are met. The module provides a capability to build polymorphic CLI&rsquo;s - sub-commands and mix-ins may override other sub-commands and mix-ins with the same name. This is similar to method overriding in Object-Oriented languages like Java. For example, a base CLI package may have a basic implementation of some sub-command. A derived package would add dependencies with advanced sub-commands to pom.xml. These sub-commands would replace (override) basic sub-commands during construction of the command hierarchy. Javadoc Contributing sub-commands In addition to the picocli way of adding sub-commands programmatically and using @Command annotation subcommands element this module provides a few more ways to contribute sub-commands which are explained below. In all cases create a sub-class of SubCommandCapabilityFactory and implement/override the following methods: getCommandType - used for declarative matching createCommand for imperative (programmatic) matching doCreateCommand: Declarative - in combination with @SubCommands or @Parent Imperative - override match() as well. Add to module-info.java: provides org.nasdanika.capability.CapabilityFactory with &lt;factory class&gt; opens &lt;sub-command package name&gt; to info.picocli, org.nasdanika.html.model.app.gen.cli; Opening to org.nasdanika.html.model.app.gen.cli is needed if you want to generate extended documentation (see below). @SubCommands annotation This one is similar to @Command.subcommands - the parent command declares types of sub-commands. However: Sub-commands are collected using the capability framework from SubCommandCapabilityFactory&rsquo;s. Sub-commands types listed in the annotation are base types - classes or interfaces - not necessarily concrete implementation types. E.g. you may have HelpCommand interface or base class and all commands implementing/extending this class will be added to the parent command. If there are two commands with the same name one of them might override the other as explained below. @Parent annotation In this case the sub-command or mix-in class are annotated with @Parent annotation listing types of parents. The sub-command/mix-in will be added to all commands in the hierarchy which are instances of the specified parent types - exact class, interface implementation, or sub-class. Programmatic match The above two ways of matching parent commands and sub-commands are handled by the SubCommandCapabilityFactory.match() method. You may override this method or createCommand() method to programmatically match parent path and decide whether to contribute a sub-command or not. Contributing mix-ins Similar to sub-commands, mix-ins can be contributed top-down and bottom-up - declaratively using annotations and programmatically. In all cased create s sub-class of MixInCapabilityFactory, implement/override: getMixInType() - for declarative matching getName() createMixIn() for imperative matching, or doCreateMixIn() Declarative - in combination with @MixIns or @Parent Imperative - override match() as well. Add to module-info.java: provides org.nasdanika.capability.CapabilityFactory with &lt;factory class&gt; opens &lt;mix-in package name&gt; to info.picocli; @MixIns annotation Mix-ins are collected using the capability framework from MixInCapabilityFactory&rsquo;s. Mix-in types listed in the annotation are base types - classes or interfaces - not necessarily concrete implementation types. @Parent annotation See &ldquo;@Parent annotation&rdquo; sub-section in &ldquo;Contributing sub-commands&rdquo; section above. Programmatic match The above two ways of matching parent commands and sub-commands/mix-ins are handled by the MixInCapabilityFactory.match() method. You may override this method or createMixIn() method to programmatically match parent path and decide whether to contribute a mix-in or not. Overriding A command/mix-in overrides another command/mix-in if: It is a sub-class of that command/mix-in It implements Overrider interface and returns true from overrides(Object other) method. It is annotated with @Overrides and the other command is an instance of one of the value classes. Extended documentation You may annotate commands with @Description to provide additional information in generated HTML site. Building distributions A distribution is a collection of modules contributing commands and mix-ins plus launcher scripts for different operating systems. org.nasdanika.cli and org.nasdanika.launcher modules are examples of building distributions as part of Maven build. Building a distribution involves two steps: Downloading modules (dependencies) Generating launcher scripts Downloading dependencies Dependencies can be downloaded using Maven dependency plug-in: &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;\n\txmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\n\txsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\n\t\n\t...\n\n\t&lt;dependencies&gt;\n\t\t...\n\t&lt;/dependencies&gt;\n\n\t&lt;build&gt;\n\t\t&lt;plugins&gt;\n\t\t\t...\n\n\t\t\t&lt;plugin&gt;\n\t\t\t\t&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n\t\t\t\t&lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;\n\t\t\t\t&lt;version&gt;3.6.1&lt;/version&gt;\n\t\t\t\t&lt;executions&gt;\n\t\t\t\t\t&lt;execution&gt;\n\t\t\t\t\t\t&lt;id&gt;copy-dependencies&lt;/id&gt;\n\t\t\t\t\t\t&lt;phase&gt;prepare-package&lt;/phase&gt;\n\t\t\t\t\t\t&lt;goals&gt;\n\t\t\t\t\t\t\t&lt;goal&gt;copy-dependencies&lt;/goal&gt;\n\t\t\t\t\t\t&lt;/goals&gt;\n\t\t\t\t\t\t&lt;configuration&gt;\n\t\t\t\t\t\t\t&lt;outputDirectory&gt;\n\t\t\t\t\t\t\t\t${project.build.directory}/dist/lib\n\t\t\t\t\t\t\t&lt;/outputDirectory&gt;\n\t\t\t\t\t\t\t&lt;useRepositoryLayout&gt;true&lt;/useRepositoryLayout&gt;\t\t\t\t\t\t\t\n\t\t\t\t\t\t&lt;/configuration&gt;\n\t\t\t\t\t&lt;/execution&gt;\n\t\t\t\t&lt;/executions&gt;\n\t\t\t&lt;/plugin&gt;\n\t\t\t\n\t\t\t...\n\t&lt;/build&gt;\n\n\t...   \n&lt;/project&gt;\n Generating launcher scripts Launcher scripts can be generated using launcher command. The command can be issued manually from the command line. Alternatively, you can execute the launcher command from an integration test as shown below: public class BuildDistributionIT {\n\t\t\n\t@Test\n\tpublic void generateLauncher() throws IOException {\n\t\tfor (File tf: new File(&quot;target&quot;).listFiles()) {\n\t\t\tif (tf.getName().endsWith(&quot;.jar&quot;) &amp;&amp; !tf.getName().endsWith(&quot;-sources.jar&quot;) &amp;&amp; !tf.getName().endsWith(&quot;-javadoc.jar&quot;)) {\n\t\t\t\tFiles.copy(\n\t\t\t\t\t\ttf.toPath(), \n\t\t\t\t\t\tnew File(new File(&quot;target/dist/lib&quot;), tf.getName()).toPath(), \n\t\t\t\t\t\tStandardCopyOption.REPLACE_EXISTING);\t\t\n\t\t\t}\n\t\t}\t\t\n\t\t\n\t\tModuleLayer layer = Application.class.getModule().getLayer();\n\t\ttry (Writer writer = new FileWriter(new File(&quot;target/dist/modules&quot;))) {\n\t\t\tfor (String name: layer.modules().stream().map(Module::getName).sorted().toList()) {\n\t\t\t\twriter.write(name);\n\t\t\t\twriter.write(System.lineSeparator());\n\t\t\t};\n\t\t}\n\t\t\n\t\tCommandLine launcherCommandLine = new CommandLine(new LauncherCommand());\n\t\tlauncherCommandLine.execute(\n\t\t\t\t&quot;-b&quot;, &quot;target/dist&quot;, \n\t\t\t\t&quot;-M&quot;, &quot;target/dist/modules&quot;, \n\t\t\t\t&quot;-f&quot;, &quot;options&quot;,\n\t\t\t\t&quot;-j&quot;, &quot;@java&quot;,\n\t\t\t\t&quot;-o&quot;, &quot;nsd.bat&quot;);\n\t\t\n\t\tlauncherCommandLine.execute(\n\t\t\t\t&quot;-b&quot;, &quot;target/dist&quot;, \n\t\t\t\t&quot;-M&quot;, &quot;target/dist/modules&quot;, \n\t\t\t\t&quot;-j&quot;, &quot;#!/bin/bash\\n\\njava&quot;,\n\t\t\t\t&quot;-o&quot;, &quot;nsd&quot;,\n\t\t\t\t&quot;-p&quot;, &quot;:&quot;,\n\t\t\t\t&quot;-a&quot;, &quot;$@&quot;);\t\t\n\t\t\n\t}\n\n}\n If the Maven project which builds the distribution does not contribute its own code, then the for loop copying the jar file can be omitted."},"core/resources/index.html":{"path":"Core/Resources","action-uuid":"1dbef1a0-faca-4afd-897a-a85c9c977928","title":"Resources","content":"Javadoc"},"practices/index.html":{"action-uuid":"7c74aa76-2a4a-4773-8a24-ab743ed62ac2","title":"Practices","content":"Practices explain how to use Nasdanika products to achieve specific goals and explain why particular design choices wer made. The enterprise model provides deeper insight on the WHY in general. The practices are organized into an enterprise continuum from the most generic on the left to the most specific on the right. However, the most specific on the right is still generic and needs to be specialized for a particular application (embodiment): Analysis, Visualization &amp; Generation - describes a general approach on using Nasdanika products. Java Analysis, Visualization &amp; Generation - application of the above to the Java model1 Loading and analyzing Java sources and bytecode, generation of non-Java artifacts such as HTML reports Generation of Java sources. JUnit test generation for low coverage methods - further specialization of the Java practice to identify methods with low test coverage using the Coverage Model and then generate JUnit tests for those methods using the Java model and OpenAI. You can think of the three practices above as progressive &ldquo;binding of decision&rdquo; as you move from the left to the right to reach &ldquo;executability&rdquo; - ability to deliver value. A java analogy for progressive specialization would be incremental binding of generic types as exemplified below: Map&lt;K,V&gt; - generic map. MyMap&lt;K extends Comparable&gt; extends Map&lt;&lt;K, MyValue&lt;K&gt;&gt; - the above map bound to a single generic parameter with an upper bound. It is a specialization of the above generic map which is also generic. Some decisions were bound, but there are still decisions to be bound. MyMap&lt;String&gt; theMap = ...; - fully bound map. Decisions are bound at variation point. For example, &ldquo;storage&rdquo; is a variation point, &ldquo;blob storage&rdquo; is one of alternatives, decision to use &ldquo;blob storage&rdquo; binds the variation point to a specific alternative. Decision binding forms a graph. Once you bind, say, &ldquo;storage&rdquo; variation point, some downstream alternatives may become unavailable because they are incompatible with that binding. Some might be available, but make no sense. For example, a decision to send data unencrypted over a public network is compatible with a decision to purchase some additional strong encryption hardware to use on-prem, but does it make business sense? Different alternatives feature different &ldquo;quality attributes&rdquo; - performance, reliability, cost. As the number of variation points and alternatives grows purely human-based decision making becomes inefficient. In this case variation points can be modeled as requirements and alternatives as capability providers or capabilities with quality attributes (seecapability). After this a list of &ldquo;designs&rdquo; (a.k.a. &ldquo;provisioning plans&rdquo;) can be created. A design/provisioning plan is a collection of compatible capabilities. If a list of designs is short enough it can be analyzed by humans directly. In the case of long lists or a large number of very similar designs decision analysis can be employed for making a selection of a design which is best fit for purpose. The page provides a general overview and the book goes into more details. â†©"},"core/emf/index.html":{"path":"Core/EMF","action-uuid":"1eaffc71-3ff2-4fa3-b2db-7558f297e603","title":"EMF","content":"Javadoc"},"nsd-cli/nsd/index.html":{"path":"CLI/nsd","action-uuid":"6606a632-01b6-4a96-987a-082eddb3cf84","title":"nsd","content":"Version: org.nasdanika.cli@2024.5.0 \r\nUsage: nsd [-hV] COMMAND\r\nNasdanika Command Line Interface\r\n  -h, --help      Show this help message and exit.\r\n  -V, --version   Print version information and exit.\r\nCommands:\r\n  launcher     Generates Java command line from directories of modules/jars\r\n  app          HTML Application model commands\r\n  help         Outputs usage for all registred commands\r\n  http-server  Serves HTTP routes\r\n  java         Commands related to Java"},"html/models/html/index.html":{"path":"HTML/Models/HTML","action-uuid":"163e6836-4432-460a-80f5-99eea15f24c6","title":"HTML","content":"TODO"},"index.html":{"action-uuid":"8aecfa62-8e9d-4548-9cdc-59bfae3de6d4","title":"Nasdanika","content":" Common Resources Persistence Ncore Diagram Graph Drawio EMF Exec Maven Capability CLI HTTP Core HTML HTML Bootstrap App Models JsTree Bootstrap EMF HTML GitLab Family Architecture Git Excel ECharts Nature Bank PDF Party Coverage Source Engineering Java Maven Enterprise Function Flow Rules Azure Decision Analysis Capability Flow Ecore Jira Models Data Sources Loader Store Key Extractor Query Engine Requestor Generator Responder Retrieval Augmented Generation (RAG) Analysis, Visualization &amp; Generation Java Analysis, Visualization &amp; Generation JUnit Tests Generation Practices Beyond Diagrams Java Analysis, Visualization, and Generation Books CLI Medium Publication Common Resources Persistence Ncore Diagram Graph Drawio EMF Exec Maven Capability CLI HTTP Core HTML HTML Bootstrap App Models JsTree Bootstrap EMF HTML GitLab Family Architecture Git Excel ECharts Nature Bank PDF Party Coverage Source Engineering Java Maven Enterprise Function Flow Rules Azure Decision Analysis Capability Flow Ecore Jira Models Data Sources Loader Store Key Extractor Query Engine Requestor Generator Responder Retrieval Augmented Generation (RAG) Analysis, Visualization &amp; Generation Java Analysis, Visualization &amp; Generation JUnit Tests Generation Practices Beyond Diagrams Java Analysis, Visualization, and Generation Books CLI Medium Publication"},"html/jstree/index.html":{"path":"HTML/JsTree","action-uuid":"ee57e0d4-dc09-4105-a6ed-07f5eeb4d7f6","title":"JsTree","content":"TODO"},"nsd-cli/nsd/app/site/index.html":{"path":"CLI/nsd/app/site","action-uuid":"7283f6fa-e0f5-48cf-92f4-1d1a156eab2a","title":"site","content":"Version: org.nasdanika.html.model.app.gen.cli@2024.5.0 \r\nUsage: nsd app site [-dhjlV] [-b=&lt;baseDir&gt;] [-m=&lt;domian&gt;] [-p=&lt;progressOutput&gt;]\r\n                    [-P=&lt;parallelism&gt;] [-r=&lt;pageErrors&gt;] [-t=&lt;timeout&gt;]\r\n                    [-T=&lt;pageTemplate&gt;] [-w=&lt;workDir&gt;] [-c=&lt;String=String&gt;]...\r\n                    [-C=URL]... [-M=&lt;String=String&gt;]... [-e[=&lt;excludes&gt;...]]...\r\n                    [-i[=&lt;includes&gt;...]]... &lt;model&gt; &lt;output&gt;\r\nGenerates HTML site\r\n      &lt;model&gt;                Model URI, resolved relative\r\n                             to the current directory\r\n      &lt;output&gt;               Output directory\r\n  -b, --base-dir=&lt;baseDir&gt;   Base directory\r\n  -c, --context-entry=&lt;String=String&gt;\r\n                             Context entries.\r\n                             Shadow entries in contexts and mounts.\r\n  -C, --context=URL          Context resource URL relative to the current\r\n                               directory. YAML, JSON, or properties. In\r\n                               properties dots are treated as key path\r\n                               separators. Type is inferred from the content\r\n                               type header, if it is present, or extension.\r\n                               Contexts are composed in the order of\r\n                               definition, later context entries shadowing the\r\n                               former\r\n  -d, --data                 Output progress data\r\n  -e, --exclude[=&lt;excludes&gt;...]\r\n                             Output directory clean excludes\r\n                             Ant pattern\r\n  -h, --help                 Show this help message and exit.\r\n  -i, --include[=&lt;includes&gt;...]\r\n                             Output directory clean includes\r\n                             Ant pattern\r\n  -j, --json                 Output progress in JSON\r\n  -l, --[no-]clean           Clean working directory\r\n                             defaults to true\r\n  -m, --domain=&lt;domian&gt;      Sitemap domain\r\n  -M, --context-mount=&lt;String=String&gt;\r\n                             MappingContext resource URL relative to the\r\n                               current directory. YAML, JSON, or properties. In\r\n                               properties dots are treated as key path\r\n                               separators. Type is inferred from the content\r\n                               type header, if it is present, or extension.\r\n                               Mounts shadow context entries.\r\n  -p, --progress=&lt;progressOutput&gt;\r\n                             Output file for progress monitor\r\n  -P, --parallelism=&lt;parallelism&gt;\r\n                             If the value greater than one then an executor\r\n                               service is created and injected into the context\r\n                               to allow concurrent execution.\r\n  -r, --errors=&lt;pageErrors&gt;  Expected number of page errors\r\n  -t, --timeout=&lt;timeout&gt;    If parallelism is greater than one this option\r\n                               specifies timout in seconds awaiting completion\r\n                               of execution. Default value is 60.\r\n  -T, --page-template=&lt;pageTemplate&gt;\r\n                             Page template URI relative\r\n                             to the current directory\r\n  -V, --version              Print version information and exit.\r\n  -w, --work-dir=&lt;workDir&gt;   Working directory\r\nExit codes:\r\n  Non-negative number   Delegate result\r\n  -1                    Unhandled exception during execution\r\n  -2                    Invalid input\r\n  -3                    Diagnostic failed\r\n  -4                    Execution failed or was cancelled, successful rollback\r\n  -5                    Execution failed or was cancelled, rollback failed\r\n  -6                    Executor service termination timed out"},"glossary.html":{"action-uuid":"49c0385c-c9ec-4f4a-bddd-de781eb084b9","title":"Glossary","content":"Clear Identifier(s) Hide UUID {{data.value.name}} {{data.value[0].value}} {{item.value}}"},"core/common/index.html":{"path":"Core/Common","action-uuid":"4fbba5cc-3093-4a84-b6ae-d889d8fca0a9","title":"Common","content":"Javadoc"},"practices/junit/index.html":{"path":"Practices/JUnit Tests Generation","action-uuid":"4384ac2f-fe14-4754-90d4-45b1832e2556","title":"JUnit Tests Generation","content":"This practice is a specialization of the Java Analysis, Visualization &amp; Generation Practice for generation of JUnit tests. In particular: Generation of tests for methods or classes with low test coverage Leveraging Gen AI such as OpenAI ChatGPT or Azure OpenAI Service for test generation The above diagram shows Java development activities and artifacts. Black arrows show the typical process, blue arrows show the test generation loop. The developer produces source artifacts which may include non-java artifacts used to generate Java code (e.g. Ecore models), &ldquo;main&rdquo; Java sources and test Java sources. Java sources are compiled into bytecode (class files). Here it is important to note that matching of bytecode classes and methods to source code classes and methods might be non-trivial because of: Lambdas Anonymous and method-scope classes Annotation processors like Lombok JUnit tests are compiled and executed. If code coverage, such as jacoco, is configured then test execution produces coverage data. Jacoco stores coverage data in jacoco.exec file. This file is used to generate a coverage report and upload coverage information to systems like SonarQube. In this practice it is also used to select which methods to generate tests for based on coverage data. This diagram provides an insight into the test generation activity: Coverage data and bytecode are used as input to load the Coverage model. Source files, the coverage model, and bytecode (optional) are used to load the Java model of source code. The generator traverses the model and generates unit tests for method with low coverage using a combination of programmatic (traditional) generation and Gen AI. Tests are generated as a Java model as well and then are delivered to the developer for review, modification, and inclusion into the unit test suite. The following section provides an overview of two &ldquo;local loop&rdquo; reference implementations (a.k.a. designs/embodiments) - all-in-one and componentized. There are many possible designs leveraging different alternatives at multiple variation points. The sections after the reference implementations section provide an overview of variation points, alternatives, and factors to take into consideration during alternative selection. Command line Reference Implementations All-in-one Componentized Repository scan/crawl Variation points and alternatives Stakeholders &amp; Activities Developer Build machine Test generator GenAI Messages Channels Developer -&gt; Build Machine/Test Generation : Source code Build Machine -&gt; Test Generator : Coverage results, possibly with bytecode Test Generation -&gt; Developer : Generated tests Test Generation - GenAI : Prompt Command line Nasdanika CLI features JUnit command which generates JUnit tests as explained above. Reference Implementations This section explains reference implementations All-in-one All-in-one generations is implemented as a JUnit test is available in TestGenerator. An example of tests generated by this generator - PetControllerTests. As the name implies, all steps of source analysis and generation are implemented in a single class and are executed in one go. Componentized Componentized test generation which is also executed in one go is implemented in these classes: TestJavaAnalyzers - loads sources, coverage, and inspectors, passes the sources to the inspectors, aggregates and saves results. Coverage Inspector - generates tests for methods with low coverage leveraging TestGenerator capability provided by OpenAITestGenerator. Repository scan/crawl TestGitLab demonstrates how to scan a source repository (GitLab) using REST API, inspect code, generate unit tests, commit them to the server (also over the REST API) and create a merge request. This implementation does not use coverage information, its purpose is to demonstrate operation over the REST API without having to clone a repository, which might be an expensive operation. The implementation uses GitLab Model to communicate with the repository. It uses Java model to load sources and StringBuilder to build test cases. Variation points and alternatives As you have seen above, you can have an AI-powered JUnit test generator in about 230 lines of code, and maybe it would all you need. However, there are many variation points (design dimensions), alternatives at each point and, as such, possible permutations of thereof (designs). This section provides a high level overview of variation points and alternatives. How to assemble a solution from those alternative is specific to your context and there might be different solutions for different contexts and multiple solutions complementing each other. As you proceed with assembling a solution, or a portfolio of solutions, you may identify more variation points and alternatives. To manage the complexity you may use: Enterprise Model for general guidance, Capability framework or Capability model to create a catalog of variation points and alternatives and compute solutions (designs) from them Decision Analysis to select from the computed list of designs Flow to map your development process AS-IS and then augment it with test generation activities at different points. In this section we&rsquo;ll use the below diagram and the concept of an Enterprise with Stakeholders performing activities and exchanging Messages over Channels. The mission of our enterprise is to deliver quality Java code. The loss function to minimize is loss function = cost * risk / business value. For our purposes we&rsquo;ll define risk as inversely proportional to tests coverage risk = missed lines / total lines - that&rsquo;s all we can measure in this simple model. The cost includes resources costs - salary, usage fees for OpenAI. Below is a summary of our enterprise: Stakeholders &amp; Activities: Developer - writes code Build machine - compiles code and executes tests Test generator - generates unit tests GenAI - leveraged by the Test Generator Messages: Source code Bytecode Coverage results Prompt to generate a test Generated tests Channels Developer -&gt; Build Machine : Source code Developer -&gt; Test Generation : Source code Build Machine -&gt; Test Generator : Coverage results, possibly with bytecode Test Generation -&gt; Developer : Generated tests Test Generation - GenAI : Prompt The below sections outline variation points and alternatives for the list items above Stakeholders &amp; Activities Developer A developer writes code - both &ldquo;business&rdquo; and test. They use some kind of an editor, likely an IDE - Eclipse, IntelliJ, VS Code. Different IDE&rsquo;s come with different sets of plug-ins, including AI assistants. Forcing a developer to switch from their IDE of preference to another IDE is likely to cause considerable productivity drop, at least for some period of time, even if the new IDE is considered superior to the old IDE. So, if you want to switch to another IDE just because it has some plug-in which you like - think twice. Build machine A build machine compiles code and executes tests. Technically, compilation and test execution may be separated in two individual activities. We are not doing it for this analysis because it doesn&rsquo;t carry much relevance to test generation. You can do it for yours. Test generator Test generator generates tests by &ldquo;looking&rdquo; at the source code, bytecode, and code coverage results. Because the source code is a model element representing piece of code (method, constructor, &hellip;), the generator may traverse the model to &ldquo;understand&rdquo; the context. E.g. it may take a look at the method&rsquo;s class, other classes in the module. If the sources are loaded from a version control system, it may take a look at the commits. And if the source model is part of an organization model, it may look at &ldquo;sibling&rdquo; modules and other resources. By analyzing source and bytecode the generator would know methods a given method calls, objects it creates, and also it would know methods calling the method. It will also &ldquo;know&rdquo; branch conditions, e.g. switch cases. Using this information the generator may: Generate comments to help the developer Generate mocks, including constructor and static methods mocks Generate tests for different branches Build a variety of prompts for GenAI The test generator may do the following code generated by GenAI: Add to generated test methods commented out - as it is done in the reference implementations &ldquo;Massage&rdquo; - remove backticks, parse, add imports - generated and implied. In addition to code generation the generator may ask GenAI to explain code and generate recommendations - it will help the developer to understand the source method and possibly improve it along the way. It may also generate dependency graphs and sequence diagrams. GenAI There may GenAI models out there - cloud, self hosted. Which one to use heavily depends on the context. For example, if you have a large codebase with considerable amount of technical debt having an on-prem model may be a good choice because: You may fine-tune it. Even if you don&rsquo;t have tons of GPU power and your model is relatively slow you can crawl you code base, generate tests and deliver them to developers for review and inclusion into test suites. In this scenario your cost is on-prem infrastructure and power. Your savings are not having to pay for GenAI in the cloud and developer productivity if your fined tuned model turns out to be more efficient than a &ldquo;vanilla&rdquo; LLM. There are many other considerations, of course! Messages In this section we&rsquo;ll take a look just at bytecode and coverage results delivered to the test generator. The generator operates on models. As such, bytecode and coverage results can be delivered in a &ldquo;raw&rdquo; format to be loaded to a model by the generator, or pre-loaded to a model and saved to a file. The second option results in fewer files to pass to the test generator. The model file can be in XMI format or in compressed binary. The XMI format is human-readable, the binary format takes less space on disk. Channels Developer -&gt; Build Machine/Test Generation : Source code For local development the build machine is the same machine where developer creates sources. The test generator is also executed on the developer&rsquo;s workstation. As such, the delivery channels is the file system. In the case of CI/CD pipeline/build server such as Jenkins or GitHub Actions, a version control systems is the delivery channel. Build Machine -&gt; Test Generator : Coverage results, possibly with bytecode The test generator needs coverage results. If the coverage results are delivered in the raw form, it also needs bytecode (class files) to make sense of the results. Coverage results can be delivered to the test generator using the following channels: Filesystem Jenkins workspace made available to the test generator over HTTP(S) Binary repository. For example, coverage results might be published to the Maven repository as an assembly along with sources, jar file, and javadoc. They can be published in a raw format or as a model. In this modality the tests generator can get everything it needs from a Maven repository. You can use Maven model or Maven Artifact Resolver API to work with Maven repositories. See also Apache Maven Artifact Resolver. Additional value of storing coverage data in a binary repository is that it can serve as an evidence of code quality stored with the compiled code, not in some other system. Source repository. Traditionally storing derived artifacts in a source repository is frowned upon. However, storage is cheap, GitHub Pages use this approach - so, whatever floats your boat! SonarQube - it doesn&rsquo;t store method level coverage, so the solution would have to operate on the class level and generate test methods for all methods in a class with low coverage. You may have a specialized application/model repository/database and store coverage information there, possibly aligned to your organization structure. Test Generation -&gt; Developer : Generated tests The goal is to deliver generated tests to the developer, make the developer aware that they are available, and possibly track progress of incorporating the generated tests into the test suite. With this in mind, there are the following alternatives/options: Filesystem - for the local loop Source control system - commit, create a merge/pull request. When using this channels you can check if there is already a generated test and whether it needs to be re-generated. If, say, the source method hasn&rsquo;t changed (the same SHA digest), and the generator version and configuration hasn&rsquo;t changed - do not re-generate, it will only consume resources and create &ldquo;noise&rdquo; - the LLM may return a different response, developers will have to spend time understanding what has changed. You may fork a repository instead of creating a branch. This way all work on tests will be done in the forked repository and the source repository will receive pull requests with fully functional tests. Tests can be generated to a separate directory and then copied to the source directory, or they can be generated directly to the source directory. Tests may be generated with @Disabled annotation so they are clearly visible in the test execution tree, and with @Generated annotation to track changes and merge generated and hand-crafted code. Issue tracking system - either attach generated tests to issues, or create a merge request and reference it from the generated issues. In systems like Jira you may create a hierarchy of issues (epic/story), assign components, labels, fix versions, assignees, etc. You may assign different generated tests to different developers based on size and complexity of the source method. E.g. tests for short methods with low complexity can be assigned to junior developers. This alone may give your team a productivity boost! E-mail or other messaging system. Issue trackers and messaging systems may be used to deliver generated documentation while source control will deliver generated tests. Developers will use the generated documentation such as graphs, sequence diagrams and GenAI explanations/recommendations in conjunction with the generated test code. This channel may implement some sort of backpressure by saying &ldquo;it is enough for now&rdquo;, as a human developer would by crying &ldquo;Enough is enough, I have other stories to work on in this sprint!&rdquo;. Generating just enough tests is beneficial in the following ways: Does not overwhelm developers Does not result in a stale generated code waiting to be taken a look at Does not waste resources and time to generate code which nobody would look at in the near future Uses the latest (and hopefully the greatest) generator version With backpressure a question of prioritization/sorting arises - what to work on first? Source methods can be sorted according to: Size/complexity Dependency. E.g. method b (caller) calls method a (callee) One strategy might be to work on callee methods first (method a) to provide a solid foundation. Another is to work on caller methods first because callee methods might be tested along the way. These strategies might be combined - some developers (say junior) may work on callee tests and senior developers may be assigned to test (complex) caller (top level) methods. Also, the top-down approach (callers first) might be better for addressing technical debt accrued over time, while bottom-up (callees first) for new development. Test Generation - GenAI : Prompt GenAI is neither free not blazing fast. As such, this channel may implement: Billing Rate limiting (throttling) Budgeting - so many calls per time period Caching Java Sources Source Artifacts Bytecode Coverage Data Developer JUnit Tests Gen AI Code Generation Compilation Test Execution JUnit Test Generation Coverage Report Java Sources Bytecode Coverage Data Developer Gen AI Coverage Model Source Code Model Tests Model Generator JUnit Test Generation Writing Code Compilation Testing Test Generation Gen AI"},"nsd-cli/index.html":{"action-uuid":"e1538cc3-95e4-4730-a5be-b7e04c646519","title":"CLI","content":"Nasdanika Command Line Interface (CLI) is a suite of Nasdanika capabilities packaged as command line tools. Prerequisites To run Nasdanika CLI you&rsquo;d need Java 17+. To build from sources you&rsquo;d also need Maven. Installation Download installation archive from the releases page. On Linux make nsd executable: chmod a+x nsd. Building from sources Download sources as a zip file or clone the repository Run mvn clean verify After the build completes the distribuion will be available in target/dist directory Adding to PATH The distribution is portable and local - it can be put to any directory, but it can only be executed from that directory. To create an installation which can be used from any directory you will need to create launcher files with absolute paths. Windows nsd.bat launcher -f options-global -o nsd-global.bat -s -m org.nasdanika.launcher -c org.nasdanika.launcher.Launcher -M modules -j &quot;@java&quot;\n Add the installation to the PATH environment variable. You may delete/rename nsd.bat and rename nsd-global.bat to nsd.bat. Linux ./nsd launcher -o nsd-global -s -m org.nasdanika.launcher -c org.nasdanika.launcher.Launcher -M modules\n Open nsd-global in a text editor and add #!/bin/bash line before the java command line. Make the file executable and add the installation directory to the path. You may remove/rename nsd and rename nsd-global to nsd. If you get java.lang.module.FindException: Module &lt;module name&gt; not found error, open the file in a text editor, locate the problematic module and remove it from the --add-modules list."},"core/drawio/index.html":{"path":"Core/Drawio","action-uuid":"ea4c8bfe-e8e2-4b2e-95ba-90c7aa5a6cf4","title":"Drawio","content":"Nasdankia provides two Maven modules for working with Drawio diagrams - API and Model. The modules require Java 17 or above. API Drawio module provides Java API for reading and manipulating Drawio diagrams. It is built on top of Graph. The module provides the following interfaces representing elements of a diagram file: Document - the root object of the API representing a file/resource which contains one or more pages. Page - a page containing a diagram (Model). Model - a diagram model containing the diagram root. Root - the root of the model containing layers. Layer - a diagram may have one or more layers. Layers contain Nodes and Connections. Node - a node can be connected to other nodes with connections. A node may contain other nodes and connections. Connection - a connection between two nodes. The below diagram shows relationships between the above interfaces including their super-interfaces: Util provides utility methods such as layout() and methods to navigate and query documents and their elements. Model Drawio Model module provides an EMF Ecore model for diagrams. A model instance can be obtained from the API document by calling Document.toModelDocument() method. The model makes it more convenient to work with the diagram elements by: Making a reference between pages and model elements bi-directional. Introducing Tag class as opposed to a string in the API. Tag is contained by Page and has bi-directional reference with tagged elements. Document The root object of the API representing a file/resource which contains one or more pages Page A page containing a diagram (Model) Model A diagram model containing the diagram root Root The root of the model containing layers Layer A diagram may have one or more layers. Layers contain Nodes and Connections. Link [0..1] Layer Element Element Model Element Node A node can be connected to other nodes with connections. A node may contain other nodes and connections. Connection A connection between two nodes * source 0..1 outgoingConnections * 1 1 1..* * target 0..1 incomingConnections * Tag * * *"},"core/exec/index.html":{"path":"Core/Exec","action-uuid":"c79f7e5b-cb5c-4b55-b133-03c40a986eae","title":"Exec","content":"Javadoc"},"nsd-cli/nsd/java/junit/index.html":{"path":"CLI/nsd/java/junit","action-uuid":"1331611e-39b6-41ec-83ad-a5d84bd8749a","title":"junit","content":"Version: org.nasdanika.models.java.cli@2024.5.0 \r\nUsage: nsd java junit [-dhjVw] [--[no-]ai] [--[no-]comment-response]\r\n                      [--disabled] [--api-endpoint=&lt;apiEndpoint&gt;]\r\n                      [-c=&lt;classes&gt;] [--class-suffix=&lt;classSuffix&gt;]\r\n                      [-J=&lt;jacoco&gt;] [-k=&lt;apiKey&gt;] [-l=&lt;limit&gt;]\r\n                      [-m=&lt;deploymentOrModelName&gt;] [-p=&lt;progressOutput&gt;]\r\n                      [--package-suffix=&lt;packageSuffix&gt;] [-r=&lt;prompt&gt;]\r\n                      [-s=&lt;sources&gt;] [-t=&lt;coverageType&gt;]\r\n                      [-v=&lt;apiKeyEnvironmentVariable&gt;] [-e[=&lt;excludes&gt;...]]...\r\n                      [-i[=&lt;includes&gt;...]]... &lt;projectDir&gt; &lt;coverageThreshold&gt;\r\n                      &lt;output&gt;\r\nGenerates JUnit tests\r\n      &lt;projectDir&gt;          Project directory\r\n      &lt;coverageThreshold&gt;   Coverage threshold\r\n      &lt;output&gt;              Output directory\r\n                            relative to the project directory\r\n      --[no-]ai             Use AI, defaults to true\r\n      --api-endpoint=&lt;apiEndpoint&gt;\r\n                            OpenAPI endpoint, defaults to\r\n                            https://api.openai.com/v1/chat/completions\r\n  -c, --classes=&lt;classes&gt;   Classes directory path relative\r\n                            to the project directory,\r\n                            defaults to target/classes\r\n      --class-suffix=&lt;classSuffix&gt;\r\n                            Test class suffix\r\n                            defaults to Tests\r\n      --[no-]comment-response\r\n                            Comment AI responses\r\n                            defaults to true\r\n  -d, --data                Output progress data\r\n      --disabled            Generate disabled tests\r\n  -e, --exclude[=&lt;excludes&gt;...]\r\n                            Source excludes\r\n                            Ant pattern\r\n  -h, --help                Show this help message and exit.\r\n  -i, --include[=&lt;includes&gt;...]\r\n                            Source includes\r\n                            Ant pattern\r\n  -j, --json                Output progress in JSON\r\n  -J, --jacoco=&lt;jacoco&gt;     jacoco.exec file path relative\r\n                            to the project directory,\r\n                            defaults to target/jacoco.exec\r\n  -k, --api-key=&lt;apiKey&gt;    OpenAPI key\r\n  -l, --limit=&lt;limit&gt;       Maximum number of test classes\r\n                            to generate\r\n  -m, --model=&lt;deploymentOrModelName&gt;\r\n                            OpenAPI deployment or model\r\n                            defaults to gpt-4\r\n  -p, --progress=&lt;progressOutput&gt;\r\n                            Output file for progress monitor\r\n      --package-suffix=&lt;packageSuffix&gt;\r\n                            Test package suffix\r\n                            defaults to .tests\r\n  -r, --prompt=&lt;prompt&gt;     Propmt\r\n                            defaults to 'Generate a JUnit 5 test method\r\n                              leveraging Mockito for the following Java method'\r\n  -s, --sources=&lt;sources&gt;   Sources directory path relative\r\n                            to the project directory,\r\n                            defaults to src/main/java\r\n  -t, --coverage-type=&lt;coverageType&gt;\r\n                            Coverage type\r\n                            Valid values: complexity, instruction, branch, line\r\n                            defaults to line\r\n  -v, --api-key-variable=&lt;apiKeyEnvironmentVariable&gt;\r\n                            OpenAPI key environment variable\r\n                            defaults to OPENAI_API_KEY\r\n  -V, --version             Print version information and exit.\r\n  -w, --overwrite           Overwrite existing tests"},"html/html/index.html":{"path":"HTML/HTML","action-uuid":"91f0d866-d1af-4106-b453-52d6e0d3a4a9","title":"HTML","content":"TODO"},"nsd-cli/nsd/java/index.html":{"path":"CLI/nsd/java","action-uuid":"7e86718c-86ce-42c9-b762-aacb532c510a","title":"java","content":"Version: org.nasdanika.models.java.cli@2024.5.0 \r\nUsage: nsd java [-hV] [COMMAND]\r\nCommands related to Java\r\n  -h, --help      Show this help message and exit.\r\n  -V, --version   Print version information and exit.\r\nCommands:\r\n  junit  Generates JUnit tests"},"nsd-cli/nsd/help/site/index.html":{"path":"CLI/nsd/help/site","action-uuid":"29ab1c67-0fb7-426b-b234-0e65337e49d6","title":"site","content":"Usage: nsd help site [-dhjlV] [-b=&lt;baseDir&gt;] [-m=&lt;domian&gt;]\r\n                     [-p=&lt;progressOutput&gt;] [-P=&lt;parallelism&gt;] [-r=&lt;pageErrors&gt;]\r\n                     [--root-action-icon=&lt;rootActionIcon&gt;]\r\n                     [--root-action-location=&lt;rootActionLocation&gt;]\r\n                     [--root-action-text=&lt;rootActionText&gt;] [-t=&lt;timeout&gt;]\r\n                     [-T=&lt;pageTemplate&gt;] [-w=&lt;workDir&gt;] [-c=&lt;String=String&gt;]...\r\n                     [-C=URL]... [-M=&lt;String=String&gt;]... [-e\r\n                     [=&lt;excludes&gt;...]]... [-i[=&lt;includes&gt;...]]... &lt;output&gt;\r\nGenerates help HTML site\r\n      &lt;output&gt;               Output directory\r\n  -b, --base-dir=&lt;baseDir&gt;   Base directory\r\n  -c, --context-entry=&lt;String=String&gt;\r\n                             Context entries.\r\n                             Shadow entries in contexts and mounts.\r\n  -C, --context=URL          Context resource URL relative to the current\r\n                               directory. YAML, JSON, or properties. In\r\n                               properties dots are treated as key path\r\n                               separators. Type is inferred from the content\r\n                               type header, if it is present, or extension.\r\n                               Contexts are composed in the order of\r\n                               definition, later context entries shadowing the\r\n                               former\r\n  -d, --data                 Output progress data\r\n  -e, --exclude[=&lt;excludes&gt;...]\r\n                             Output directory clean excludes\r\n                             Ant pattern\r\n  -h, --help                 Show this help message and exit.\r\n  -i, --include[=&lt;includes&gt;...]\r\n                             Output directory clean includes\r\n                             Ant pattern\r\n  -j, --json                 Output progress in JSON\r\n  -l, --[no-]clean           Clean working directory\r\n                             defaults to true\r\n  -m, --domain=&lt;domian&gt;      Sitemap domain\r\n  -M, --context-mount=&lt;String=String&gt;\r\n                             MappingContext resource URL relative to the\r\n                               current directory. YAML, JSON, or properties. In\r\n                               properties dots are treated as key path\r\n                               separators. Type is inferred from the content\r\n                               type header, if it is present, or extension.\r\n                               Mounts shadow context entries.\r\n  -p, --progress=&lt;progressOutput&gt;\r\n                             Output file for progress monitor\r\n  -P, --parallelism=&lt;parallelism&gt;\r\n                             If the value greater than one then an executor\r\n                               service is created and injected into the context\r\n                               to allow concurrent execution.\r\n  -r, --errors=&lt;pageErrors&gt;  Expected number of page errors\r\n      --root-action-icon=&lt;rootActionIcon&gt;\r\n                             Root action icon\r\n      --root-action-location=&lt;rootActionLocation&gt;\r\n                             Root action location\r\n      --root-action-text=&lt;rootActionText&gt;\r\n                             Root action text\r\n  -t, --timeout=&lt;timeout&gt;    If parallelism is greater than one this option\r\n                               specifies timout in seconds awaiting completion\r\n                               of execution. Default value is 60.\r\n  -T, --page-template=&lt;pageTemplate&gt;\r\n                             Page template URI relative\r\n                             to the current directory\r\n  -V, --version              Print version information and exit.\r\n  -w, --work-dir=&lt;workDir&gt;   Working directory\r\nExit codes:\r\n  Non-negative number   Delegate result\r\n  -1                    Unhandled exception during execution\r\n  -2                    Invalid input\r\n  -3                    Diagnostic failed\r\n  -4                    Execution failed or was cancelled, successful rollback\r\n  -5                    Execution failed or was cancelled, rollback failed\r\n  -6                    Executor service termination timed out"},"practices/java/index.html":{"path":"Practices/Java Analysis, Visualization &amp; Generation","action-uuid":"a501093d-dc25-4abf-bb1b-2b1794e94b18","title":"Java Analysis, Visualization &amp; Generation","content":"This practice is a specialization of the Analysis, Visualization &amp; Generation Practice for using the Java model as a source model, target model, or both. This page provides a high level reference and the book goes into details. So what is possible to do with the Java model/language in addition to generic analysis, visualization and generation? Analysis Java model can be loaded from sources and bytecode. Tests coverage can be loaded from jacoco.exec and class files and associated with model elements. Bytecode information can be used to establish bi-directional references between model elements - field access, method calls. Bytecode can be instrumented to collect runtime cross-referencing such as reflective method calls and field access. Visualization Module, package, class, method dependency graphs. The graphs may reflect coverage data so they can be used for prioritization of addressing technical debt. For example, many well-covered microservices may use a shared library with low coverage. Sequence diagrams Generation Documentation Documentation similar to documentation generated from Ecore models such as Java model above, Family model, or Enterprise model with: * Visualizations mentioned above\n* Documentation produced by GenAI - explainations and recommendations.\n Such documentation may be useful in modernization efforts where there is a need to understand a legacy codebase. It may also be useful in onboarding of new team members and it might help provide deeper insights into the codebase for all team members. Source code Source code with @Generated annotations or @generated Javadoc tags to allow detection of changes in the generated code and re-generation only if there changes in the generator inputs, and the output was not modified since the last generation. It allows concurrent evolution of the generator, generator inputs, and manual modifications. For more details see Solution instantiation. RAG/Chat RAG/Chat on top of the Java model may use bytecode and runtime introspection information in addition to just source code. For example &ldquo;This method is overridden by &hellip; and is called by &hellip;&rdquo;. RAG may be contextual - chat with a class, a method, a package, a module or an application (group of modules) if the model elements are &ldquo;mounted&rdquo; under higher level constructs such as products and segments."},"core/capability/index.html":{"path":"Core/Capability","action-uuid":"998386e8-d229-4621-a535-b1257d171883","title":"Capability","content":"Nasdanika Capability framework1 allows to discover/load capabilities which meet a requirement. Capabilities are provided by CapabilityFactory create() method. Capability factories may request other capabilities they need. As such, capabilities can be chained. Factories create CapabilityLoaders which provide Flux reactive streams of capabilities. It allows to have an infinite stream of capabilities which are consumed (and produced) as needed. Capability providers may furnish additional information about capabilities. This information can be used for filtering or sorting providers. A non-technical example of requirement/capability chain graph is a food chain/graph. Food is a requirement. Or &ldquo;I want to eat&rdquo; is a requirement. Bread and, say fried eggs are two capabilities meeting/addressing the requirement. Bread requires &ldquo;wheat&rdquo;, &ldquo;water&rdquo;, and &ldquo;bake&rdquo; capabilities. Fried eggs require &ldquo;egg&rdquo;, &ldquo;oil&rdquo;, and &ldquo;fry&rdquo; capabilities. Bread capability provider may implement Vegan marker interface which can be used for filtering. All food capabilities may implement NutritionalInformation interface - it can be used for filtering or sorting. A more technical example is Java ServiceLoader with service type being a requirement and an instance of the service class being a capability. Nasdanika capability framework can operate on top of ServiceLoader and may be thought of as a generalization of service loading. In essence, the capability framework is a backward chaining engine as shown in one of the example below. Client code - requesting a capability Capabilities are loaded by CapabilityLoader. Capability loader can take an iterable of capability factories in its constructor, or it can load them using ServiceLoader as shown in the below code snippet: CapabilityLoader capabilityLoader = new CapabilityLoader();\ncapabilityLoader.getFactories().add(new TestServiceFactory&lt;Object&gt;());\nProgressMonitor progressMonitor = new PrintStreamProgressMonitor();\n\t\t\nfor (CapabilityProvider&lt;?&gt; cp: capabilityLoader.load(new TestCapabilityFactory.Requirement(&quot;Hello World&quot;), progressMonitor)) {\n\tSystem.out.println(cp);\n\tFlux&lt;?&gt; publisher = cp.getPublisher();\n\t\t\t\n\tpublisher.subscribe(System.out::println);\n}\n Factories can also be added post-construction with getFactories().add(factory). Service capabilities Service requirements and capabilities provide functionality similar to ServiceLoader - requesting instances of specific type, but extend it with ability to provide additional service requirement. This functionality is provided by ServiceCapabilityFactory and ServiceCapabilityFactory.Requirement. CapabilityLoader capabilityLoader = new CapabilityLoader();\ncapabilityLoader.getFactories().add(new TestServiceFactory&lt;Object&gt;());\nProgressMonitor progressMonitor = new PrintStreamProgressMonitor();\n\t\t\n@SuppressWarnings({ &quot;unchecked&quot;, &quot;rawtypes&quot; })\nServiceCapabilityFactory.Requirement&lt;List&lt;Double&gt;, Double&gt; requirement = (ServiceCapabilityFactory.Requirement) ServiceCapabilityFactory.createRequirement(List.class, null,  33.0);\nfor (CapabilityProvider&lt;?&gt; cp: capabilityLoader.load(requirement, progressMonitor)) {\n\tSystem.out.println(cp);\n\tFlux&lt;?&gt; publisher = cp.getPublisher();\n\t\t\t\n\tpublisher.subscribe(System.out::println);\n}\n It is also possible to load services from ServiceLoader using subclasses of Service. You&rsquo;d need to subclass ServiceFactory in a module which uses a particular service and override stream(Class&lt;S&gt; service) method as shown below: @Override\nprotected Stream&lt;Provider&lt;S&gt;&gt; stream(Class&lt;S&gt; service) {\n\treturn ServiceLoader.load(service).stream();\n}\n Then you&rsquo;d need to add the factory to the loader: capabilityLoader.getFactories().add(new TestServiceFactory&lt;Object&gt;());\n Providing a capability As it was mentioned above, capability factories can be explicitly added to CapabilityLoader or loaded using ServiceLoader. Below is an example of a capability factory: public class TestCapabilityFactory implements CapabilityFactory&lt;TestCapabilityFactory.Requirement, Integer&gt; {\n\t\n\tpublic record Requirement(String value){};\n\t\n\t@Override\n\tpublic boolean canHandle(Object requirement) {\n\t\treturn requirement instanceof Requirement;\n\t}\n\n\t@Override\n\tpublic CompletionStage&lt;Iterable&lt;CapabilityProvider&lt;Integer&gt;&gt;&gt; create(\n\t\t\tRequirement requirement,\n\t\t\tBiFunction&lt;Object, ProgressMonitor, CompletionStage&lt;Iterable&lt;CapabilityProvider&lt;Object&gt;&gt;&gt;&gt; resolver,\n\t\t\tProgressMonitor progressMonitor) {\n\t\t\n\t\treturn resolver.apply(MyService.class, progressMonitor).thenApply(cp -&gt; {;\n\t\t\t@SuppressWarnings({ &quot;unchecked&quot;, &quot;rawtypes&quot; })\n\t\t\tFlux&lt;MyService&gt; myServiceCapabilityPublisher = (Flux) cp.iterator().next().getPublisher();\n\t\t\t\n\t\t\treturn Collections.singleton(new CapabilityProvider&lt;Integer&gt;() {\n\t\n\t\t\t\t@Override\n\t\t\t\tpublic Flux&lt;Integer&gt; getPublisher() {\n\t\t\t\t\tFunction&lt;MyService, Integer&gt; mapper = ms -&gt; ms.count(((Requirement) requirement).value());\n\t\t\t\t\treturn myServiceCapabilityPublisher.map(mapper);\n\t\t\t\t}\n\t\t\t\t\n\t\t\t});\n\t\t});\n\t}\n\n}\n Applications Services Service capabilities explained above a used by Graph and Function Flow for loading node processors and connection processors for a specific requirement using NodeProcessorFactory and ConnectionProcessorFactory respectively. For example, code generation, execution, simulation. Solutions for architectures One of future application of the capability framework is creation a list of solution alternatives for an architecture/pattern. For example, there might be multiple RAG embodiments with different key types, key extractors, stores, &hellip; Some of &ldquo;design dimensions&rdquo; are listed below: Key type: Bag of words. Multiple options - just words, words with frequency, tokenized words, word stems. Embedding vector - different embedding models, different dimensions. Store - multiple stores for multiple key types. Multiple indexing and retrieval methods. Chunk size, chunk overlap, chunking algorithm. Generator - multiple models and prompts As you can see a number of potential combinations can easily go into thousands or even be infinite. Reactive approach with filtering and sorting may be helpful in selecting a solution which is a good fit for a particular use case - number and type of data sources etc. For example, if the total size of data is under a few gigabytes an in-memory store may be a better choice than, say, an external (vector) database. Also an old good bag of words might be better than embeddings. E.g. it might be cheaper. Solution alternatives may include temporal aspect or monetary aspects. For example, version X of Y is available at time Z. Z might be absolute or relative. Say, Z days after project kick-off or license fee payment. Identified solutions meeting requirements can have different quality attributes - costs (to build, to run), timeline, etc. These quality attributes can be used for solution analysis. E.g. one solution can be selected as a transition architecture and another as the target architecture. Backward chaining Family reasoning demonstrates application of the capability framework as a backward chaining engine. Family relationships such as grandfather and cousin are constructed by requiring and combining relationships such as child and sibling. Stream processing This possible application is similar to backward reasoning. Imagine an algorithmic trading strategy which uses several technical indicators, such as moving averages, to make trading decisions. Such a strategy would submit requirements for technical indicators which would include symbol, indicator configuration, time frame size. Technical indicators in turn would submit a requirement for raw trading data. A technical indicator such as moving average would start publishing its events once it receives enough trading data frames to compute its average. A trading engine would submit a requirement for strategies. A strategy factory may produce multiple strategies with different configurations. The trading engine would perform &ldquo;paper&rdquo; trades, select well-performing strategies and discard ones which perform poorly. This can be an ongoing process - if a strategy deteriorates then it is discarded and a new strategy is requested from strategy publishers - this process can be infinite. AI model training/fine-tuning This application is similar to stream processing and may be combined with backward reasoning. Let&rsquo;s say we want to train a model to answer questions about family relationships for a specific family. For example, &ldquo;Who is Alan&rsquo;s great grandmother?&rdquo; A single relationship in the model can be expressed in multiple ways in natural language. And multiple relationships can be expressed in a single sentence. For example: Elias is a person Elias is a man Elias is a male Elias is a parent of Fiona Fiona is a child of Elias Elias is a father of Fiona Fiona is a daughter of Elias Paul and Isa are parents of Lea and Elias &hellip; So, on top of a model there might be a collection of text generators. Output of those generators can be fed to a model: Supervised - question and answer &ldquo;How many sisters does Bryan have?&rdquo; - &ldquo;Two&rdquo; &ldquo;Who are Bryan&rsquo;s sisters?&rdquo; - &ldquo;Clara and Fiona&rdquo; Unsupervised - factual statements A similar approach can be applied to other models - customer/accounts, organization or architecture model, etc. For example, from the Internet Banking System we can generate something like &ldquo;Accounts Summary Controller uses Mainframe Banking System Facade to make API calls to the Mainframe Banking System over XML/HTTPS&rdquo;. &ldquo;make API calls&rdquo; may also be generated as &ldquo;connect&rdquo; or &ldquo;make requests&rdquo;. In a similar fashion a number of questions/answers can be generated. Javadoc â†©"},"core/graph/index.html":{"path":"Core/Graph","action-uuid":"473fc480-6294-48f0-8369-6fcc5b71b1d7","title":"Graph","content":"Javadoc"},"core/index.html":{"action-uuid":"2ae92ebd-9c60-4341-b4c3-7a4361d09a32","title":"Core","content":"TODO"},"html/models/index.html":{"path":"HTML/Models","action-uuid":"4f4c586b-8c39-4ee3-9381-c1efa6d2464b","title":"Models","content":"TODO"},"practices/generic/index.html":{"path":"Practices/Analysis, Visualization &amp; Generation","action-uuid":"3a027e34-7f04-4bc2-8894-25e8e08fc0d9","title":"Analysis, Visualization &amp; Generation","content":"This practice explains how to use Nasdanika products (specifically models) and related products. The above diagram shows the core idea - load input data into a model, modify the model or create a new model from it, and save the models to native (raw) formats. Loading to a model as opposed to working with raw formats gives the following benefits: Unified API Generated model documentation with visualizations Different models may extend classes from core models and be treated similarly Model classes may be subclassed and mixed Cross-reference model elements URI handlers allows to load models from diverse sources On-demand loading of resources and features of model elements Conversion of models to graphs and making them executable with graph processors E.g. want to read/write Excel files - take a look at the Excel metamodel and then use Ecore API to work with the model. Now want to work with PDF? A different metamodel, the same model API. You have Java sources stored in GitLab and want model elements to reflect both Java and GitLab natures of your sources? Create a GitLabRepositoryCompilationUnit class which extends both Compilation Unit and Repository File. Customize Loader to create this class for repository files with java extension. Want to load a PDF file directly from GitLab without having to clone the entire repository? Use GitLabURIHandler! The below diagram illustrates the above concepts: Models can be visualized using: ECharts using the ECharts model, ECharts-Java or by directly generating JavaScript/JSON. Example. PlantUML using DiagramGenerator, the diagram module or by directly generating PlantUML text and calling Plant UML API&rsquo;s. Example. Holistic model of an organization One use case for the modeling approach outlined above is creation of a holistic model of an organization/corporation as exemplified by the below diagram1 In a corporation different elements of the model are typically stored in different systems and documents like Excel spreadsheets. The modeling approach allows to load those elements in a single resource set and cross-reference them. Elements which are not stored in structured formats can be captured by modeling them in diagrams and mapping those diagrams to models, see Beyond Diagrams. One important reason why a holistic model might be beneficial for an organization is the ability of using it for AI insights. For example, using RAG/Chat on top of the organization model. Such chat can be made context-aware, chatting with the Operations will return result relevant to operations. The above diagram is very simple, a large organization may have many layers, thousands of applications, millions of lines of code. A model for such an organization would take some time to build, but it can be built incrementally - department by department, application by application. The value of building such a model will grow exponentially as more and more elements are added due to the network effect. While the resulting model might be &ldquo;large&rdquo;, &hellip; define large. Experiments show that a model element in a model like the above takes ~ 500 bytes of RAM. As such, 1 GB of RAM would hold about 2 million model elements. Also, model resources are loaded on demand, so only the model elements needed by some task would be loaded to complete that task. With DynamicDelegate it is possible to have model elements loading their data from multiple sources on demand. The organization model can be built on top of existing &ldquo;generic&rdquo; models such as Java, Maven, GitLab, Azure, &hellip; Resources TOGAF Enterprise Metamodel Corporate structure â†© Output Model Transformation Input Model Raw Input Raw Output Cell Excel Resource Paragraph PDF Resource Resource Set GitLab URI Handler MS Excel Workbook GitLab Excel Resource Factory PDF Resource Factory PDF File File system Corporation Marketing Finance Uses Operations HR Builds IT Execution environments Binary packages Source Code Application"},"nsd-cli/nsd/http-server/index.html":{"path":"CLI/nsd/http-server","action-uuid":"f696eb74-b66f-4875-b85f-55ff7105744a","title":"http-server","content":"Usage: nsd http-server [-hV] [--http-host=&lt;httpHost&gt;] [--http-port=&lt;httpPort&gt;]\r\n                       [--http-server-shutdown-timeout=&lt;timeout&gt;]\r\nServes HTTP routes\r\n  -h, --help      Show this help message and exit.\r\n      --http-host=&lt;httpHost&gt;\r\n                  HTTP host (network interface) to bind to\r\n      --http-port=&lt;httpPort&gt;\r\n                  HTTP port. If a port is not specified,\r\n                  an ephemeral port is used\r\n      --http-server-shutdown-timeout=&lt;timeout&gt;\r\n                  Timeout in seconds,\r\n                  defaults to 3 seconds\r\n  -V, --version   Print version information and exit."},"core/http/index.html":{"path":"Core/HTTP","action-uuid":"1ff8f30d-fefa-47b2-a9f6-9c16eed22890","title":"HTTP","content":"Javadoc"},"nsd-cli/nsd/help/index.html":{"path":"CLI/nsd/help","action-uuid":"8f232118-5be2-405c-8ffe-00c8f37676dd","title":"help","content":"Usage: nsd help [-ahHV] [-l=&lt;level&gt;] [-o=&lt;output&gt;] [COMMAND]\r\nOutputs usage for all registred commands\r\n  -a, --action-model      Output to action model\r\n  -h, --help              Show this help message and exit.\r\n  -H, --html              Output to HTML\r\n  -l, --header-level=&lt;level&gt;\r\n                          Starting level for HTML header tags in HTML output,\r\n                            the default value is 1.\r\n  -o, --output=&lt;output&gt;   Output file\r\n  -V, --version           Print version information and exit.\r\nCommands:\r\n  site  Generates help HTML site"}}