This practice is a specialization of the [Java Analysis, Visualization & Generation Practice](../java/index.html) for generation of [JUnit](https://junit.org) tests. 
In particular:

* Generation of tests for methods or classes with low [test coverage](https://en.wikipedia.org/wiki/Code_coverage)
* Leveraging [Gen AI](https://en.wikipedia.org/wiki/Generative_artificial_intelligence) such as [OpenAI ChatGPT](https://openai.com/chatgpt) or [Azure OpenAI Service](https://azure.microsoft.com/en-us/products/ai-services/openai-service) for test generation

```drawio-resource
practices/junit/junit-practice.drawio
```

The above diagram shows Java development activities and artifacts. 
Black arrows show the typical process, blue arrows show the test generation loop.

The developer produces source artifacts which may include non-java artifacts used to generate Java code (e.g. Ecore models), "main" Java sources and test Java sources. 
Java sources are compiled into [bytecode](https://en.wikipedia.org/wiki/Bytecode) (class files).
Here it is important to note that matching of bytecode classes and methods to source code classes and methods might be non-trivial because of:

* Lambdas
* Anonymous and method-scope classes
* Annotation processors like [Lombok](https://projectlombok.org/) 
 
JUnit tests are compiled and executed. 
If code coverage, such as [jacoco](https://www.jacoco.org/jacoco/), is configured then test execution produces coverage data. 
Jacoco stores coverage data in ``jacoco.exec`` file.
This file is used to generate a coverage report and upload coverage information to systems like [SonarQube](https://www.sonarsource.com/products/sonarqube/). 
In this practice it is also used to select which methods to generate tests for based on coverage data.

```drawio-resource
practices/junit/test-generation.drawio
```

This diagram provides an insight into the test generation activity:

* Coverage data and bytecode are used as input to load the [Coverage model](https://coverage.models.nasdanika.org/).
* Source files, the coverage model, and bytecode (optional) are used to load the [Java model](https://java.models.nasdanika.org/) of source code. 
* The generator traverses the model and generates unit tests for method with low coverage using a combination of programmatic (traditional) generation and Gen AI. Tests are generated as a Java model as well and then are delivered to the developer for review, modification, and inclusion into the unit test suite.

The following section provides an overview of two "local loop" reference implementations (a.k.a. designs/embodiments) - all-in-one and componentized.
There are many possible designs leveraging different alternatives at multiple variation points. 
The sections after the reference implementations section provide an overview of variation points, alternatives, and factors to take into consideration during alternative selection.

----

[TOC levels=6]


## Reference Implementations

This section explains two reference implementations

### All-in-one

All-in-one generations is implemented as a JUnit test is available in [TestGenerator](https://github.com/Nasdanika-Models/rules/blob/main/tests/analyzer/src/test/java/org/nasdanika/models/rules/tests/analyzer/tests/TestGenerator.java).
An example of tests generated by this generator - [PetControllerTests](https://github.com/Nasdanika/spring-petclinic-test-generation/blob/main/generated-tests/org/springframework/samples/petclinic/owner/tests/PetControllerTests.java).

As the name implies, all steps of source analysis and generation are implemented in a single class and are executed in one go.

### Componentized

Componentized test generation which is also executed in one go is implemented in these classes:

* [TestJavaAnalyzers](https://github.com/Nasdanika-Models/rules/blob/main/tests/analyzer/src/test/java/org/nasdanika/models/rules/tests/analyzer/tests/TestJavaAnalyzers.java) - loads sources, coverage, and [inspectors](https://github.com/Nasdanika-Models/rules/blob/main/model/src/main/java/org/nasdanika/models/rules/reflection/Inspector.java), passes the sources to the inspectors, aggregates and saves results.
* [Coverage Inspector](https://github.com/Nasdanika-Models/rules/blob/main/tests/inspectors/src/main/java/org/nasdanika/models/rules/tests/inspectors/JavaCoverageReflectiveInspectors.java) - generates tests for methods with low coverage leveraging [TestGenerator](https://github.com/Nasdanika-Models/rules/blob/main/tests/inspectors/src/main/java/org/nasdanika/models/rules/tests/inspectors/TestGenerator.java) [capability](../../core/capability/index.html) provided by [OpenAITestGenerator](https://github.com/Nasdanika-Models/rules/blob/main/tests/inspectors/src/main/java/org/nasdanika/models/rules/tests/inspectors/OpenAITestGenerator.java).

## Variation points and alternatives

As you have seen above, you can have an AI-powered JUnit test generator in about 230 lines of code, and maybe it would all you need.
However, there are many variation points (design dimensions), alternatives at each point and, as such, possible permutations of thereof (designs).
This section provides a high level overview of variation points and alternatives. 
How to assemble a solution from those alternative is specific to your context and there might be different solutions for different contexts and multiple solutions complementing each other. 
As you proceed with assembling a solution, or a portfolio of solutions, you may identify more variation points and alternatives.
To manage the complexity you may use:

* [Enterprise Model](https://enterprise.models.nasdanika.org/) for general guidance, 
* [Capability framework](../../core/capability/index.html) or [Capability model](https://capability.models.nasdanika.org/) to create a catalog of variation points and alternatives and compute solutions (designs) from them
* [Decision Analysis](https://mcda.models.nasdanika.org/) to select from the computed list of designs
* [Flow](https://flow.models.nasdanika.org/) to map your development process AS-IS and then augment it with test generation activities at different points. 

In this section we'll use the below diagram and the concept of an [Enterprise](https://enterprise.models.nasdanika.org/references/eClassifiers/Enterprise/index.html) with [Stakeholders](https://enterprise.models.nasdanika.org/references/eClassifiers/Stakeholder/index.html) performing activities and exchanging [Messages](https://enterprise.models.nasdanika.org/references/eClassifiers/Message/index.html) over [Channels](https://enterprise.models.nasdanika.org/references/eClassifiers/Channel/index.html). 

```drawio-resource
practices/junit/test-generation.drawio
```

The mission of our enterprise is to deliver quality Java code. 
The loss function to minimize is ``loss function = cost * risk / business value``.
For our purposes we'll define risk as inversely proportional to tests coverage ``risk = missed lines / total lines`` - that's all we can measure in this simple model.
The cost includes resources costs - salary, usage fees for OpenAI.

Below is a summary of our enterprise:

* Stakeholders & Activities:
    * Developer - writes code
    * Build machine - compiles code and executes tests
    * Test generator - generates unit tests
    * GenAI - leveraged by the Test Generator
* Messages:
    * Source code
    * Bytecode
    * Coverage results
    * Prompt to generate a test
    * Generated tests
* Channels
    * Developer -> Build Machine : Source code
    * Developer -> Test Generation : Source code
    * Build Machine -> Test Generator : Coverage results, possibly with bytecode
    * Test Generation -> Developer : Generated tests      
    * Test Generation - GenAI : Prompt   

The below sections outline variation points and alternatives for the list items above

### Stakeholders & Activities

#### Developer

A developer writes code - both "business" and test. 
They use some kind of an editor, likely an IDE - Eclipse, IntelliJ, VS Code.
Different IDE's come with different sets of plug-ins, including AI assistants. 
Forcing a developer to switch from their IDE of preference to another IDE is likely to cause considerable 
productivity drop, at least for some period of time, even if the new IDE is considered superior to the old IDE.
So, if you want to switch to another IDE just because it has some plug-in which you like - think twice.

#### Build machine

A build machine compiles code and executes tests. 
Technically, compilation and test execution may be separated in two individual activities. 
We are not doing it for this analysis because it doesn't carry much relevance to test generation. 
You can do it for yours.

#### Test generator

Test generator generates tests by "looking" at the source code, bytecode, and code coverage results.

Because the source code is a model element representing piece of code ([method](https://java.models.nasdanika.org/references/eClassifiers/Method/index.html), [constructor](https://java.models.nasdanika.org/references/eClassifiers/Constructor/index.html), ...), the generator may traverse the model to "understand" the context. 
E.g. it may take a look at the method's class, other classes in the module. 
If the sources are loaded from a version control system, it may take a look at the commits.
And if the source model is part of an organization model, it may look at "sibling" modules and other resources.
Using this information it may build a wide variety of prompts for GenAI.

By analyzing source and bytecode the generator would know which methods a given method calls, which objects it creates, and also it would know methods calling the method.
It will also "know" branch conditions, e.g. switch cases.
Using this information the generator may:

* Generate comments to help the developer
* Generate mocks, including constructor and static methods mocks
* Generate tests for different branches

#### GenAI

There may GenAI models out there - cloud, self hosted.
Which one to use heavily depends on the context. 
For example, if you have a large codebase with considerable amount of technical debt having an on-prem model may be a good choice because:

* You may fine-tune it.
* Even if you don't have tons of GPU power and your model is relatively slow you can crawl you code base, generate tests and deliver them to developers for review and inclusion into test suites.

In this scenario your cost is on-prem infrastructure and power. 
Your savings are not having to pay for GenAI in the cloud and developer productivity if your fined tuned model turns out to be more efficient than a "vanilla" LLM.

There are many other considerations, of course!

### Messages

In this section we'll take a look just at bytecode and coverage results delivered to the test generator.
The generator operates on models.
As such, bytecode and coverage results can be delivered in a "raw" format to be loaded to a model by the generator, 
or pre-loaded to a model and saved to a file.
The second option results in fewer files to pass to the test generator.
The model file can be in XMI format or in compressed binary.
The XMI format is human-readable, the binary format takes less space on disk.

### Channels

#### Developer -> Build Machine/Test Generation : Source code

For local development the build machine is the same machine where developer creates sources.
The test generator is also executed on the developer's workstation.
As such, the delivery channels is the file system.

In the case of CI/CD pipeline/build server such as [Jenkins](https://www.jenkins.io/) or GitHub Actions, a version control systems is the delivery channel. 

#### Build Machine -> Test Generator : Coverage results, possibly with bytecode

The test generator needs coverage results.
If the coverage results are delivered in the raw form, it also needs bytecode (class files) to make sense of the results.

Coverage results can be delivered to the test generator using the following channels:

* Filesystem
* Jenkins workspace made available to the test generator over HTTP(S)
* Binary repository. For example, coverage results might be published to the Maven repository as an assembly along with sources, jar file, and javadoc. They can be published in a raw format or as a model. In this modality the tests generator can get everything it needs from a Maven repository. 
* TODO - specialized repo for coverage info 

#### Test Generation -> Developer : Generated tests      

Filtering, throttling, jira

#### Test Generation - GenAI : Prompt   

Billing, throttling, caching, authentication, ...






### Tests generator

### Delivery of generated tests

### Test generation execution

Maven plugin, pipeline, crawling

### Access to artifacts

Jenkins workspace, Maven repository, Maven classloader / artifacts loader - reference Maven model



----


TODO: Diagram (site - activities, artifacts, variation points), manually created sources, generated source, bytecode, coverage report. Lombok, ecore. 



Sorting (by dependency, size, ...), throttling, creation of work items

Comparison, positioning, complementary - availability of IDE plug-ins/extensions in org. Not better, different.

IDE plug-ins: select code, type a request, wait for generation, save to a file, add package declaration and some imports.

dependency:

* top-down - downstream dependencies might be tested along the way
* bottom-up

both have merits and might be applicable in diffent situations. bottom-up for new development, top-down for addressing technical debt.


## Parking lot

* Mention GitLab URI handler - get from, push to, create a merge reguest, link to sources
* Fine-tuned models (?) Llama 2?
* Massaging returned text - backticks, imports - explicit and implicit, use Java parser/model.
* Pet clinic demo - G/H fork, use GPT 4, specify SpringBoot and Mockito in prompt, tags - as is/disabled test methods
* Use with G/H copilot - complementary, different
* Unit tests delivery - branch, fork
* SonarQube - class level. 
* jacoco.exec - Maven repository - evidence for other purposes. Coverage storage. Jenkins workspace
* Mention assembly of a solution from alternatives, use capability and decision analysis.
* Coverage - jacoco.exec + class files, sq api, JSON, DB/REST API (part of the enterprise model, mention the generic thing), XMI/Binary. Storage - local, Jenkins workspace, sq, maven binary repository
* Mock opportunities - we know methods being called, constructors etc. We can generate mock generation and try/catch for static methods and constructors.
* Branches, switch cases in particular and enums
* Generate review/explain 
* @Mock, @InjectMocks
* Cost value for each acitivity, opportunities
* Sort by dependency, lower-level may be (partially) testes as part of higher level
* Prompt building - eContainer, resource, resource set (on-demand loading), mention enterprise model from the generic
* Massaging generated sources - peeling back-ticks, parsing, imports - in the snippet, implicit/assumed.
* All in one code snippet, rules - diagram and snippets
* Generation - lombok
* Recipe book 
* Loops - local, pipeline, issues, scan repos, pull/merge request, sq
* TODO - sequence diagram for the componentized
* Re-generation: add @Generated annotation, source digest, generated source digest. Regenerate if source digest changed, but generated digest hasn't
* Assign different generated tests to different people by complexity/size and skill level.
* Build API delivery - throttling (rate limit), budgeting, billing
* Tech debt
* Prompt libraries/generators - context
